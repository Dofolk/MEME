{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50ccc3-870f-4ace-a7db-9a65cdcd8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First version for training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe298c98-8f09-47a5-b315-3cf40df810e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "targets\n",
    "    size: batch_size(15) x fi(52 = 50 + eos + bos) x fj(4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9936738-7f48-45ea-9ec0-7feab89e3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn, tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
    "    TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn.functional import softmax\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2f2c35f-9daf-4b2e-8052-23d607ad4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 500\n",
    "nhead = 8\n",
    "encoder_layer_nums = 6\n",
    "decoder_layer_nums = 6\n",
    "dim_ff = 512\n",
    "# token: ATCG(1~4), <BOS or SOS>(0), <EOS>(16), <PAD>(17)\n",
    "    # combinations: AT, AC, AG, TC, TG, CG, ATC, ATG, ACG, TCG, ATCG(5~15, total 11)\n",
    "ntoken = 18\n",
    "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e61c159-59c5-479a-8345-01315ddffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #done\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        encoder_layer_nums,\n",
    "        decoder_layer_nums,\n",
    "        dim_ff,\n",
    "        ntoken,\n",
    "    ):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pos_encode = PositionalEmbedding(d_model, dropout, max_len)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n",
    "        self.src_embedding = nn.Embedding(ntoken, d_model, scale_grad_by_freq = False)\n",
    "        self.tgt_embedding = nn.Embedding(4, d_model, scale_grad_by_freq = False)\n",
    "        self.out_embed = nn.Embedding(ntoken, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, ntoken)\n",
    "        self.output_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask = None,\n",
    "        tgt_mask = None,\n",
    "        memory_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None,\n",
    "        memory_key_padding_mask = None,\n",
    "    ):\n",
    "        \n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        \n",
    "        src = self.pos_encode(src)\n",
    "        tgt = self.pos_encode(tgt)\n",
    "        \n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, \n",
    "                              tgt_mask = tgt_mask, \n",
    "                              memory_mask = memory_mask, \n",
    "                              tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask = memory_key_padding_mask\n",
    "                             )\n",
    "        output = self.output_linear(output)\n",
    "        output = self.output_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3f8c252-87a3-4ed0-87c3-f78e0e8763a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): #done\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be0cc8-9e23-4799-aab7-1f9cc63c3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_loss = 0.\n",
    "times = 100\n",
    "\n",
    "for step in range(1):\n",
    "    data, target = get_batch()\n",
    "    I = indicator(data)\n",
    "    data_key_padding_mask = get_padding_mask(data)\n",
    "    target_key_padding_mask = get_padding_mask(target)\n",
    "    target_mask = nn.Transformer.generate_square_subsequent_mask(target.size(-1))\n",
    "    last_f = None\n",
    "    for _ in range(times):###\n",
    "        output = model(data, target,\n",
    "                       src_key_padding_mask = data_key_padding_mask,\n",
    "                       tgt_key_padding_mask = target_key_padding_mask,\n",
    "                       tgt_mask = target_mask\n",
    "                      )\n",
    "        if dist(last_f, output) < epsilon:\n",
    "            break\n",
    "        loss = EM_criterion(output, I, last_f)\n",
    "        last_f = output\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6de76c93-e468-4ef0-9a91-77c44229c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(data):\n",
    "    assert isinstance(data, list), 'Input type for data is not list.'\n",
    "    length = len(data[0])\n",
    "    indicator = list()\n",
    "    for seq in data:\n",
    "        indicator.append(list(map(lambda x: alphabet_dict[x], seq)))\n",
    "        assert len(seq) == length, 'The length is different for input seq'\n",
    "\n",
    "    return tensor(indicator, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93870dd5-7701-41b6-88e8-675f0dd0fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_list_ver(data):\n",
    "    assert isinstance(data, list), 'Input type for data is not list.'\n",
    "    length = len(data[0])\n",
    "    indicator = list()\n",
    "    for seq in data:\n",
    "        indicator.append( torch.tensor(list(map(lambda x: alphabet_dict[x], seq)) ))\n",
    "        assert len(seq) == length, 'The length is different for input seq'\n",
    "\n",
    "    return indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b886f19-f52c-4c66-bbc0-bde4a3b9382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fij_x_indicator(f, I):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f858345-7351-4f39-a25a-524203fbfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c990b-ff52-4bfd-8552-a637451056b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_criterion(new_output, I, last_output = None, sub_len, last_lambda):\n",
    "    assert new_output.shape[0] == last_output.shape[0],\\\n",
    "                'The batch size for last and new output unequal.'\n",
    "    batch_size = new_output.shape[0]\n",
    "    if torch.is_tensor(I):\n",
    "        I = list(I)\n",
    "\n",
    "    likelihood, nll, f, lamb = [], [], [], []\n",
    "    for batch in range(batch_size):\n",
    "        likelihood_res, nll_res, new_f, new_lamb = EM_step(I, batch, last_lamb, new_output[batch])\n",
    "        likelihood.append(likelihood_res)\n",
    "        nll.append(nll_res)\n",
    "        f.append(new_f)\n",
    "        lamb.append(new_lamb)\n",
    "    \n",
    "    return torch.sum(nll)\n",
    "\n",
    "def EM_step(I, batch, last_lamb, f_ij, update_f = True):\n",
    "\n",
    "    assert torch.is_tensor(last_lamb), \"The lambda value is not in tensor(EM_step function).\"\n",
    "    f_ij = torch.log(f_ij)\n",
    "    W = 10 + batch\n",
    "    px1 = list()\n",
    "    px2 = list()\n",
    "    n = 0\n",
    "\n",
    "    for seq in I:\n",
    "        for i in range(len(seq) - W):\n",
    "            n += 1\n",
    "            p1, p2 = 0, 0\n",
    "            for j in range(W):\n",
    "                p1 += f_ij[j][seq[i+j]]\n",
    "                p2 += f_ij[0][seq[i+j]]\n",
    "            px1.append(p1)\n",
    "            px2.append(p2)\n",
    "\n",
    "    lamb = last_lamb.tile(n,1).t() #make the lambda value into 2xn shape\n",
    "    prob = tensor([px1,px2])\n",
    "    multi = torch.multiply(prob, lamb)\n",
    "    Z = torch.div(multi, torch.sum(multi, dim = 0)) # shape: 2xn\n",
    "\n",
    "    # nll = negative log likelihood function\n",
    "    likelihood = Z * torch.log(prob) + Z * torch.log(lamb)\n",
    "    nll = -likelihood\n",
    "\n",
    "    new_lamb = Z / n\n",
    "\n",
    "    if update_f:\n",
    "        c0 = torch.zeros(1,4)\n",
    "        cj = torch.zeros(W,4)\n",
    "        for seq in I:\n",
    "            for i in range(len(seq) - W):\n",
    "                for j in range(W):\n",
    "                    for L in range(4):\n",
    "                        c0[0][L] += Z[1][i] if seq[i+j] == (L + 1) else 0\n",
    "                    cj[j][seq[i+j] - 1] += Z[0][i]\n",
    "        c0_sum = torch.sum(c0, axis = 1)\n",
    "        cj_sum = torch.sum(cj, axis = 1)\n",
    "        new_f0 = c0 / c0_sum\n",
    "        new_fij = torch.zeros(W,4)\n",
    "        for i in range(W):\n",
    "            new_fij[i] = cj[i]/cj_sum\n",
    "    \n",
    "    \n",
    "    return likelihood, nll, new_f, new_lamb\n",
    "\n",
    "def condi_distribution(f, I, n, W):\n",
    "    '''\n",
    "    Calculate the conditional distribution p(Xi | theta_j)\n",
    "    eq(7),(8) in the MEME article\n",
    "    To avoid the computation error for the digits, it will use log and exponential\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    freq_letter: the frequences for each letter in each position, size: (W + 1) x L\n",
    "                 background ( 1 x L ) + motif ( W x L )\n",
    "                 dtype: np.array\n",
    "    ====================================================\n",
    "    return: it will be the log form output\n",
    "    \n",
    "    p_Xi_1: conditional distribution of motif sequence, size: n x 1\n",
    "    p_Xi_2: conditional distribution of background, size: n x 1\n",
    "    '''\n",
    "    \n",
    "    p_Xi_1 = torch.zeros(n)\n",
    "    p_Xi_2 = torch.zeros(n)\n",
    "    f = torch.log(f)\n",
    "    f_0 = f[0]\n",
    "    f_j = f[1:]\n",
    "    \n",
    "    for subseq in range(n):\n",
    "        for pos in range(W):\n",
    "            p_Xi_1[subseq] += f_j[pos][I[subseq][pos] - 1]\n",
    "            p_Xi_2[subseq] += f_0[I[subseq][pos] - 1]\n",
    "\n",
    "    return torch.exp(p_Xi_1), torch.exp(p_Xi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d77d870d-cf62-4128-98be-9e6549e1faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    \"\"\"\n",
    "    preprocess the data into the specific form\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = 15\n",
    "    W = 30\n",
    "    dna_seq = ['GGGGATGCCGT','CTTAATGCTAC']\n",
    "    alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}\n",
    "    #dna_seq = read_file(os.path.join(fileDir, file_list[ith]))\n",
    "    #assert len(dna_seq) < batch_size, f' The (row) size of the input data is out of range(>{batch_size}).'\n",
    "    row = len(dna_seq)\n",
    "    #target = make_outputs(dna_seq, batch_size)\n",
    "    #data = np.zeros((row, 500))\n",
    "    #for idx in range(len(dna_seq)):\n",
    "    #    print(f'doing the {idx} step in for loop')\n",
    "    #    data[idx] = preprocess_seq(dna_seq[idx], 1)\n",
    "        \n",
    "    #data = expand_dim(data, 1)\n",
    "\n",
    "    row, col = len(dna_seq), len(dna_seq[0])\n",
    "    data = torch.zeros((row, col))\n",
    "    \n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            data[i][j] = alphabet_dict[dna_seq[i][j]]\n",
    "    col_expand = 16 * torch.ones((row,100 - col))\n",
    "    row_expand = 16 * torch.ones((batch_size - row, 100))\n",
    "    data = torch.cat( (torch.cat((data, col_expand), dim = 1), row_expand), dim = 0)\n",
    "\n",
    "    target = torch.zeros((batch_size, W))\n",
    "    for i in range(batch_size):\n",
    "        target[i][1:i+11] += torch.ones(i+10)\n",
    "        target[i][i+11] = 3\n",
    "        target[i][i+12:W] += 2 * torch.ones(W - 2 - i - 10)\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "080b0e82-f191-4513-9ab2-cff3fe6a3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(data, pad_num = 2):\n",
    "    '''\n",
    "    create the padding mask for data\n",
    "    return:\n",
    "    mask: the padding mask in Tensor, with size same as the input data for the function\n",
    "    '''\n",
    "    mask = torch.zeros(data.shape)\n",
    "    mask[data == pad_num] = -torch.inf\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17610ea9-6dc4-4364-86de-1aaf984dc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = get_batch()\n",
    "src = src.to(torch.int64)\n",
    "tgt = tgt.to(torch.int64)\n",
    "model = TransformerModel(d_model, dropout, max_len, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, ntoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e34776-f5a8-435d-88a6-fe5632e81135",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.LongTensor(src),torch.LongTensor(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b411982c-4f48-43e0-b10a-4fbbd99233d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2,5)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cf411f9-f5ce-43d3-b938-c47c6796ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = ['GGGGATGCCGT','CTTAATGCTAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72bcd721-9f10-4085-a3f7-95dcdbc7cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.arange(10)\n",
    "print(tmp)\n",
    "tmp.tolist()\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4245315-6131-4aef-a2d4-bbf533c35fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
