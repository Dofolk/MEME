{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The model is designed to analyze the consensus sequences in DNA sequences\n",
    "\n",
    "The input of this model will be in the 2D matrix:    \n",
    "    dimension: n x 1000, n is the amount of the input samples\n",
    "\n",
    "The output of the model will be in the 3D matrix:\n",
    "    dimension: n x 1000 x 4, n is the amoutn of the input samples\n",
    "               the meaning of the output is the position-wise appearance for each alphabets(ATCG)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
    "    TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global variables\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('','r')\n",
    "seq = {}\n",
    "\n",
    "for line in f:\n",
    "    if line.startswith('>'):\n",
    "        name = line.replace('>', '').split()[0]\n",
    "        seq[name] = ''\n",
    "    else:\n",
    "        seq[name] += line.replace('\\n','').strip()\n",
    "        \n",
    "f.close()\n",
    "\n",
    "seq_ls = []\n",
    "for name in seq:\n",
    "    seq_ls.append(seq[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8495e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #done\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        encoder_layer_nums,\n",
    "        decoder_lyaer_nums,\n",
    "        dim_ff,\n",
    "        ntoken,\n",
    "    ):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pos_encode = PositionalEmbedding(d_model, dropout, max_len)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.out_embed = nn.Embedding(ntoken, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, ntoken)\n",
    "        self.output_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask = None,\n",
    "        tgt_mask = None,\n",
    "        memory_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None,\n",
    "        memory_key_padding_mask = None,\n",
    "    ):\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        \n",
    "        src = self.pos_encode(src)\n",
    "        tgt = self.pos_encode(tgt)\n",
    "        \n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, \n",
    "                              tgt_mask = tgt_mask, \n",
    "                              memory_mask = memory_mask, \n",
    "                              tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask = memory_key_padding_mask\n",
    "                             )\n",
    "        output = self.output_linear(output)\n",
    "        output = self.output_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): #done\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b495dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_algorithm(object):\n",
    "    def __init__(self, source_data, W):\n",
    "        \n",
    "        self.source_data = source_data\n",
    "        self.data_info = self.info_get()\n",
    "        self.W = W\n",
    "        self.N = None\n",
    "        self.l = None\n",
    "        self.X = None\n",
    "        self.n = None\n",
    "        self.z = None\n",
    "        self.I = None\n",
    "        \n",
    "        self.init_variables()\n",
    "        \n",
    "        \n",
    "    def init_variables(self):\n",
    "        '''\n",
    "        initialize the followings: X, f_i, z, I\n",
    "        '''\n",
    "        \n",
    "        # record N and small L\n",
    "        self.N = len(self.source_data)\n",
    "        self.l = list()\n",
    "        for seq in self.source_data:\n",
    "            self.l.append(len(seq))\n",
    "\n",
    "        # init small z\n",
    "        z = np.ones((N, max(l)))\n",
    "        for idx in range(N):\n",
    "            z[idx][(l[idx]- self.W + 1):] = 0\n",
    "        \n",
    "        # init W-mer set X\n",
    "        X_ls = list()\n",
    "        for i in range(self.N):\n",
    "            X_ls += [ self.source_data[i][j:j+self.W] for j in range( self.l[i] - self.W + 1 ) ]\n",
    "        self.X = X_ls\n",
    "        self.n = len(self.X)\n",
    "        \n",
    "        # init indicator\n",
    "        self.I = self.indicator_function()\n",
    "    \n",
    "    \n",
    "    def indicator_function(self): #done\n",
    "        '''\n",
    "        In article, it is the I(k,a) function for eq(7), (8)\n",
    "        There will transfer the alphabets to the index\n",
    "\n",
    "        return\n",
    "        indicator: transfer input string into tensor indicator, size is n x W\n",
    "        '''\n",
    "        assert isinstance(self.X, list), 'Type of X is not list'\n",
    "        indicator = list()\n",
    "        for seq in self.X:\n",
    "            indicator.append(list(map(lambda x: alphabet_dict[x], seq)))\n",
    "\n",
    "        return np.array(indicator, dtype = 'int')\n",
    "        \n",
    "    \n",
    "    def condi_distribution(self, freq_letter):\n",
    "        '''\n",
    "        Calculate the conditional distribution p(Xi | theta_j)\n",
    "        eq(7),(8) in the MEME article\n",
    "        To avoid the computation error for the digits, it will use ln() to make it being summation\n",
    "    \n",
    "        Arguments:\n",
    "    \n",
    "        freq_letter: the frequences for each letter in each position, size: L x 4\n",
    "                     background ( 1 x 4 ) + motif ( (L-1) x 4 )\n",
    "        n: amount of the input sequences\n",
    "        W: length of the input sequences\n",
    "        ====================================================\n",
    "        return:\n",
    "    \n",
    "        p_Xi_1: conditional distribution of motif sequence, size: n x 1\n",
    "        p_Xi_2: conditional distribution of background, size: n x 1\n",
    "        '''\n",
    "    \n",
    "        p_Xi_1 = np.zeros(n)\n",
    "        p_Xi_2 = np.zeros(n)\n",
    "        f_0 = freq_letter[0]\n",
    "        f_j = freq_letter[1:]\n",
    "    \n",
    "        for subseq in range(n):\n",
    "            for pos in range(W):\n",
    "                p_Xi_1[subseq] += np.log( f_j[ self.I[subseq][pos] - 1 ] )\n",
    "                p_Xi_2[subseq] += np.log( f_0[ self.I[subseq][pos] - 1 ] )\n",
    "        \n",
    "        return [p_Xi_1, np, p_Xi_2]\n",
    "    \n",
    "    def count_letter(self): #done\n",
    "        '''\n",
    "        count the total appearance times for each alphabets\n",
    "        Arguments:\n",
    "        data: input source W-mer data, size: n x W\n",
    "        return:\n",
    "        count: counting results, size: n x 4\n",
    "        '''\n",
    "\n",
    "        count = np.zeros( (self.n, 4) , dtype = 'int')\n",
    "\n",
    "        for i in range(self.n):\n",
    "            C = Counter(self.X[i])\n",
    "            count[i][0] = C['A']\n",
    "            count[i][1] = C['T']\n",
    "            count[i][2] = C['C']\n",
    "            count[i][3] = C['G']\n",
    "\n",
    "        return count\n",
    "    \n",
    "    def update_erase(self):\n",
    "        '''\n",
    "        update the erasing values\n",
    "        in article, in the MM implementation section\n",
    "        '''\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def E_step(self, condi_dis, lamb): #done\n",
    "        '''\n",
    "        calculate the Z_ij, in article's eq(4)\n",
    "\n",
    "        Arguments:\n",
    "        condi_dis: conditional distribution, from the defined function, size: n x 2\n",
    "        lamd: probability for using models, size: 1 x 2\n",
    "        return:\n",
    "        Z: membership probability, size: n x 2\n",
    "        '''\n",
    "\n",
    "        multi_results = condi_dis * np.tile(lamb, (self.n, 1))\n",
    "        summation = np.sum(multi_results, axis = 1, keepdims = True)\n",
    "        Z = multi_results / summation\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    def M_step(self, Z, I, count):\n",
    "        '''\n",
    "        Arguments:\n",
    "        Z: membership from E-step, size: n x 2\n",
    "        I: indicator function, size: n x W\n",
    "        count: count the appearance time for each alphabet in every sequences, size: n x 4\n",
    "        '''\n",
    "\n",
    "        n, W = len(I), len(I[0])\n",
    "        Z = Z.transpose()\n",
    "        count = count.transpose()\n",
    "\n",
    "        # update lambda, eq(5)\n",
    "        lamb = np.mean(Z, axis = 0)\n",
    "\n",
    "        # update f_ij\n",
    "        # calculate the c_0k and c_jk\n",
    "        c_0k = np.zeros((1, 4))\n",
    "        c_jk = np.zeros((W, 4))\n",
    "\n",
    "        for i in range(4):\n",
    "            c_0k[0][i] = np.sum( Z[1] * count[i] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condi_distribution(freq_letter, indicator, n, W):\n",
    "    '''\n",
    "    Calculate the conditional distribution p(Xi | theta_j)\n",
    "    eq(7),(8) in the MEME article\n",
    "    To avoid the computation error for the digits, it will use ln() to make it being summation\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    freq_letter: the frequences for each letter in each position, size: L x 4\n",
    "                 background ( 1 x 4 ) + motif ( (L-1) x 4 )\n",
    "    indicator: indicator function, size: n x L\n",
    "    n: amount of the input sequences\n",
    "    W: length of the input sequences\n",
    "    ====================================================\n",
    "    return:\n",
    "    \n",
    "    p_Xi_1: conditional distribution of motif sequence, size: n x 1\n",
    "    p_Xi_2: conditional distribution of background, size: n x 1\n",
    "    '''\n",
    "    \n",
    "    p_Xi_1 = np.zeros(n)\n",
    "    p_Xi_2 = np.zeros(n)\n",
    "    f_0 = freq_letter[0]\n",
    "    f_j = freq_letter[1:]\n",
    "    \n",
    "    for subseq in range(n):\n",
    "        for pos in range(W):\n",
    "            p_Xi_1[subseq] += np.log( f_j[ indicator[subseq][pos] - 1 ] )\n",
    "            p_Xi_2[subseq] += np.log( f_0[ indicator[subseq][pos] - 1 ] )\n",
    "        \n",
    "    return p_Xi_1, np, p_Xi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_function(data): #done\n",
    "    '''\n",
    "    In article, it is the I(k,a) function for eq(7), (8)\n",
    "    There will transfer the alphabets to the index\n",
    "    \n",
    "    Arguments\n",
    "    data: input W-mer data, dtype is list of string, size: n x W\n",
    "    \n",
    "    return\n",
    "    indicator: transfer input string into tensor indicator, size is n x W\n",
    "    '''\n",
    "    \n",
    "    if len(data) <= 1 or len(data[0]) <= 1:\n",
    "        print('Error: the input data has less than 1 sequence')\n",
    "    \n",
    "    n, W = len(data), len(data[0])\n",
    "    indicator = list()\n",
    "    prev = W\n",
    "    \n",
    "    for seq in data:\n",
    "        indicator.append(list(map(lambda x: alphabet_dict[x], seq)))\n",
    "        assert len(seq) == W, \"There is a sequence has length error(>W or <W, ...>W<?)\"\n",
    "    \n",
    "    return np.array(indicator, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_membership(condi_dis, lamb): #done\n",
    "    '''\n",
    "    calculate the Z_ij, in article's eq(4)\n",
    "    \n",
    "    Arguments:\n",
    "    condi_dis: conditional distribution, from the defined function, size: n x 2\n",
    "    lamd: probability for using models, size: 1 x 2\n",
    "    return:\n",
    "    Z: membership probability, size: n x 2\n",
    "    '''\n",
    "    \n",
    "    n = len(condi_dis)\n",
    "    multi_results = condi_dis * np.tile(lamb, (n, 1))\n",
    "    summation = np.sum(multi_results, axis = 1, keepdims = True)\n",
    "    Z = multi_results / summation\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_letter(data): #done\n",
    "    '''\n",
    "    count the total appearance times for each alphabets\n",
    "    Arguments:\n",
    "    data: input source W-mer data, size: n x W\n",
    "    return:\n",
    "    count: counting results, size: n x 4\n",
    "    '''\n",
    "    \n",
    "    n, W = len(data), len(data[0])\n",
    "    \n",
    "    count = np.zeros((n,4), dtype = 'int')\n",
    "    \n",
    "    for i in range(n):\n",
    "        C = Counter(data[i])\n",
    "        count[i][0] = C['A']\n",
    "        count[i][1] = C['T']\n",
    "        count[i][2] = C['C']\n",
    "        count[i][3] = C['G']\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase(data, W):\n",
    "    '''\n",
    "    Set up the erasing factor at first\n",
    "    \n",
    "    data: the original data, denoted as Y, size: N x l_i\n",
    "    W: specify been W-mer\n",
    "    l: small L, start with list to store the length for each input sequences\n",
    "    '''\n",
    "    \n",
    "    N = len(data)\n",
    "    l = list()\n",
    "    \n",
    "    for seq in data:\n",
    "        l.append(len(seq))\n",
    "    \n",
    "    z = np.ones((N, max(l)))\n",
    "    \n",
    "    for idx in range(N):\n",
    "        z[idx][(l[idx]- W + 1):] = 0\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(Z, I, count):\n",
    "    '''\n",
    "    Arguments:\n",
    "    Z: membership from E-step, size: n x 2\n",
    "    I: indicator function, size: n x W\n",
    "    count: count the appearance time for each alphabet in every sequences, size: n x 4\n",
    "    '''\n",
    "    \n",
    "    n, W = len(I), len(I[0])\n",
    "    Z = Z.transpose()\n",
    "    count = count.transpose()\n",
    "    \n",
    "    # update lambda, eq(5)\n",
    "    lamb = np.mean(Z, axis = 0)\n",
    "    \n",
    "    # update f_ij\n",
    "    # calculate the c_0k and c_jk\n",
    "    c_0k = np.zeros((1, 4))\n",
    "    c_jk = np.zeros((W, 4))\n",
    "    \n",
    "    for i in range(4):\n",
    "        c_0k[0][i] = np.sum( Z[1] * count[i] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def posterior_computation(self):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45fe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb98840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    data_info = {amount of sequence, }\n",
    "    '''\n",
    "    # read data\n",
    "    data_info\n",
    "    # build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb11ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ea048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
