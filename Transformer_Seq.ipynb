{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba5deea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe model is designed to analyze the consensus sequences in DNA sequences\\n\\nThe input of this model will be in the 2D matrix:    \\n    dimension: n x 1000, n is the amount of the input samples\\n\\nThe output of the model will be in the 3D matrix:\\n    dimension: n x 1000 x 4, n is the amoutn of the input samples\\n               the meaning of the output is the position-wise appearance for each alphabets(ATCG)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model is designed to analyze the consensus sequences in DNA sequences\n",
    "\n",
    "The input of this model will be in the 2D matrix:    \n",
    "    dimension: n x 1000, n is the amount of the input samples\n",
    "\n",
    "The output of the model will be in the 3D matrix:\n",
    "    dimension: n x 1000 x 4, n is the amoutn of the input samples\n",
    "               the meaning of the output is the position-wise appearance for each alphabets(ATCG)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac6800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
    "    TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc9d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global variables\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('','r')\n",
    "seq = {}\n",
    "\n",
    "for line in f:\n",
    "    if line.startswith('>'):\n",
    "        name = line.replace('>', '').split()[0]\n",
    "        seq[name] = ''\n",
    "    else:\n",
    "        seq[name] += line.replace('\\n','').strip()\n",
    "        \n",
    "f.close()\n",
    "\n",
    "seq_ls = []\n",
    "for name in seq:\n",
    "    seq_ls.append(seq[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8495e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #done\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        encoder_layer_nums,\n",
    "        decoder_lyaer_nums,\n",
    "        dim_ff,\n",
    "        ntoken,\n",
    "    ):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pos_encode = PositionalEmbedding(d_model, dropout, max_len)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.out_embed = nn.Embedding(ntoken, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, ntoken)\n",
    "        self.output_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask = None,\n",
    "        tgt_mask = None,\n",
    "        memory_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None,\n",
    "        memory_key_padding_mask = None,\n",
    "    ):\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        \n",
    "        src = self.pos_encode(src)\n",
    "        tgt = self.pos_encode(tgt)\n",
    "        \n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, \n",
    "                              tgt_mask = tgt_mask, \n",
    "                              memory_mask = memory_mask, \n",
    "                              tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask = memory_key_padding_mask\n",
    "                             )\n",
    "        output = self.output_linear(output)\n",
    "        output = self.output_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): #done\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b495dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEME(object):\n",
    "    def __init__(self, source_data, W):\n",
    "        '''\n",
    "        source_data will be list of strings\n",
    "        '''\n",
    "        \n",
    "        self.source_data = source_data\n",
    "        self.W = W\n",
    "        self.L = 4\n",
    "        \n",
    "        # fixed variables\n",
    "        self.N = None\n",
    "        self.l = None\n",
    "        self.X = None\n",
    "        self.n = None\n",
    "        self.I = None\n",
    "        self.letter_counts = None\n",
    "        \n",
    "        # mutable variables, i.e. the value will be changed\n",
    "        self.p_X1 = None\n",
    "        self.p_X2 = None\n",
    "        self.Z = None\n",
    "        self.z = None\n",
    "        self.f_ij = None\n",
    "        self.e_ij = None\n",
    "        self.lambda1 = 0\n",
    "        self.lambda2 = 0\n",
    "        \n",
    "        self.init_variables()\n",
    "        \n",
    "        \n",
    "    def init_variables(self):\n",
    "        '''\n",
    "        initialize the followings: X, f_i, z, I, lambda, N, small L, \n",
    "        '''\n",
    "        \n",
    "        # record N and small L\n",
    "        self.N = len(self.source_data)\n",
    "        self.l = list()\n",
    "        for seq in self.source_data:\n",
    "            self.l.append(len(seq))\n",
    "\n",
    "        # init small z\n",
    "        self.z = np.ones((self.N, max(self.l)))\n",
    "        for idx in range(self.N):\n",
    "            self.z[idx][(self.l[idx]- self.W + 1):] = 0\n",
    "        self.e_ij = self.z.copy()\n",
    "        \n",
    "        # init W-mer set X\n",
    "        # X will be the list of strings\n",
    "        X_ls = list()\n",
    "        for i in range(self.N):\n",
    "            X_ls += [ self.source_data[i][j:j+self.W] for j in range( self.l[i] - self.W + 1 ) ]\n",
    "        self.X = X_ls\n",
    "        self.n = len(self.X)\n",
    "        \n",
    "        # init indicator\n",
    "        self.I = self.indicator_function()\n",
    "        \n",
    "        # init lambda\n",
    "        lamb_range_min = min( np.sqrt(self.N)/self.n, 1/(2*self.W) )\n",
    "        lamb_range_max = max( np.sqrt(self.N)/self.n, 1/(2*self.W) )\n",
    "        self.lambda1 = np.random.uniform(lamb_range_min, lamb_range_max)\n",
    "        self.lambda2 = 1 - self.lambda1\n",
    "        \n",
    "        # init f_ij, size: ( 1 + W ) x L\n",
    "        self.f_ij = np.zeros( ( (self.W + 1), self.L ) )\n",
    "        \n",
    "        # init letter_counts\n",
    "        self.letter_counts = self.count_letter_appearance()\n",
    "        \n",
    "        # f_i parts\n",
    "        for pos in range(self.W):\n",
    "            C = Counter(self.I.transpose()[pos])\n",
    "            length = len(self.I)\n",
    "            for val in range(4):\n",
    "                self.f_ij[pos + 1][val] = C[val + 1] / length\n",
    "        \n",
    "        # f_0 part\n",
    "        '''count_num = defaultdict(int)\n",
    "        for subseq in self.X:\n",
    "            for s in subseq:\n",
    "                count_num[s.upper()] += 1\n",
    "        total = np.concatenate(self.letter_counts).sum()\n",
    "        for letter in alphabet_dict:\n",
    "            self.f_ij[0][alphabet_dict[letter] - 1] = count_num[letter]/total\n",
    "        '''\n",
    "        mps = np.sum(self.f_ij.transpose(), axis = 1)\n",
    "        for a in range(4):\n",
    "            if mps[a] >= 1:\n",
    "                for j in range(4):\n",
    "                    self.f_ij[a][j] /= mps[a]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # conditional probabilities p_X1, p_X2\n",
    "        \n",
    "        self.p_X1, self.p_X2 = self.condi_distribution(self.f_ij)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def indicator_function(self): #done\n",
    "        '''\n",
    "        In article, it is the I(k,a) function for eq(7), (8)\n",
    "        There will transfer the alphabets to the index\n",
    "        Thus, the results look like \n",
    "        [\n",
    "        [1,2,3,4],\n",
    "        [4,3,2,1]\n",
    "        ]\n",
    "\n",
    "        return\n",
    "        indicator: transfer input string into tensor indicator, size is n x W\n",
    "        '''\n",
    "        assert isinstance(self.X, list), 'Type of X is not list'\n",
    "        indicator = list()\n",
    "        for seq in self.X:\n",
    "            indicator.append(list(map(lambda x: alphabet_dict[x], seq)))\n",
    "\n",
    "        return np.array(indicator, dtype = 'int')\n",
    "        \n",
    "    \n",
    "    def condi_distribution(self, freq_letter):\n",
    "        '''\n",
    "        Calculate the conditional distribution p(Xi | theta_j)\n",
    "        eq(7),(8) in the MEME article\n",
    "        To avoid the computation error for the digits, it will use ln() to make it being summation\n",
    "    \n",
    "        Arguments:\n",
    "    \n",
    "        freq_letter: the frequences for each letter in each position, size: (W + 1) x L\n",
    "                     background ( 1 x L ) + motif ( W x L )\n",
    "                     dtype: np.array\n",
    "        ====================================================\n",
    "        return: it will be the log form output\n",
    "    \n",
    "        p_Xi_1: conditional distribution of motif sequence, size: n x 1\n",
    "        p_Xi_2: conditional distribution of background, size: n x 1\n",
    "        '''\n",
    "    \n",
    "        p_Xi_1 = np.zeros(self.n)\n",
    "        p_Xi_2 = np.zeros(self.n)\n",
    "        f_0 = freq_letter[0]\n",
    "        f_j = freq_letter[1:]\n",
    "    \n",
    "        for subseq in range(self.n):\n",
    "            for pos in range(self.W):\n",
    "                p_Xi_1[subseq] += np.log( f_j[pos][self.I[subseq][pos] - 1] )\n",
    "                p_Xi_2[subseq] += np.log( f_0[self.I[subseq][pos]-1])\n",
    "        \n",
    "        return np.exp(p_Xi_1), np.exp(p_Xi_2)\n",
    "    \n",
    "    def count_letter_appearance(self): #done\n",
    "        '''\n",
    "        count the total appearance times for each alphabets\n",
    "        Arguments:\n",
    "        data: input source W-mer data, size: n x W\n",
    "        return:\n",
    "        count: counting results, size: n x 4\n",
    "        '''\n",
    "\n",
    "        count = np.zeros( (self.n, 4) , dtype = 'int')\n",
    "\n",
    "        for i in range(self.n):\n",
    "            C = Counter(self.X[i])\n",
    "            count[i][0] = C['A']\n",
    "            count[i][1] = C['T']\n",
    "            count[i][2] = C['C']\n",
    "            count[i][3] = C['G']\n",
    "\n",
    "        return count\n",
    "    \n",
    "    def update_erasing(self):\n",
    "        '''\n",
    "        compute the erasing values e_ij\n",
    "        '''\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            for j in range(self.l[i]):\n",
    "                lower_bound = j - self.W + 1 if j - self.W + 1 >= 0 else 0\n",
    "                vals = self.z[i][lower_bound:j]\n",
    "                if sum(vals) > 1:\n",
    "                    vals /= sum(vals)\n",
    "                if any(vals > 9.99999e-1):\n",
    "                    self.e_ij[i][j] *= 1e-6\n",
    "                else:\n",
    "                    self.e_ij[i][j] *= np.exp( sum( np.log( np.ones(len(vals)) - vals ) ) )\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def update_z(self):\n",
    "        '''\n",
    "        update the small z values\n",
    "        '''\n",
    "        \n",
    "        assert sum(self.l)-self.N*(self.W - 1) == self.n, 'Error: index division not equal for updating z.'\n",
    "        idx = 0\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.l[i] - self.W + 1):\n",
    "                self.z[i][j] = self.Z[idx][0]\n",
    "                idx += 1\n",
    "        \n",
    "        return                \n",
    "    \n",
    "    def E_step(self, condi_dis, lamb): #done\n",
    "        '''\n",
    "        calculate the Z_ij, in article's eq(4)\n",
    "        also update small z depends on the results of Z\n",
    "\n",
    "        Arguments:\n",
    "        condi_dis: conditional distribution, from the defined function, size: n x 2\n",
    "        lamd: probability for using models, size: 1 x 2\n",
    "        return:\n",
    "        Z: membership probability, size: n x 2\n",
    "        '''\n",
    "        \n",
    "        # regulate the shape\n",
    "        p = condi_dis.reshape(self.n, 2)\n",
    "        lamb = lamb.reshape(1, 2)\n",
    "\n",
    "        multi_results = p * np.tile(lamb, (self.n, 1))\n",
    "        summation = np.sum(multi_results, axis = 1, keepdims = True)\n",
    "        Z = multi_results / summation\n",
    "        self.Z = Z.copy()\n",
    "        self.update_z()\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def M_step(self):\n",
    "        '''\n",
    "        calculate the lambda and f_ij\n",
    "        \n",
    "        Arguments:\n",
    "        Z: membership from E-step, size: n x 2\n",
    "        I: indicator function, size: n x W\n",
    "        count: count the appearance time for each alphabet in every sequences, size: n x 4\n",
    "        \n",
    "        return:\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Z = self.Z.transpose()\n",
    "        count = self.letter_counts.transpose()\n",
    "\n",
    "        # update lambda, eq(5)\n",
    "        lamb = np.mean(Z, axis = 1)\n",
    "        \n",
    "        # update f_ij\n",
    "        # calculate c_0k and c_jk\n",
    "        c_0k = np.zeros((1, self.L))\n",
    "        c_jk = np.zeros((self.W, self.L))\n",
    "        \n",
    "        # calculate the c_0k\n",
    "        \n",
    "        for k in range(self.L):\n",
    "            for i in range(self.n):\n",
    "                for j in range(self.W):\n",
    "                    c_0k[0][k] += Z[1][i] if self.I[i][j] == (k + 1) else 0\n",
    "        \n",
    "        '''for i in range(4):\n",
    "            c_0k[0][i] = np.sum( Z[1] * count[i] )'''\n",
    "        \n",
    "        # calculate the c_jk\n",
    "        # first make erasing being 1 x n\n",
    "        E = np.hstack([self.e_ij[i][:(self.l[i] - self.W + 1)] for i in range(self.N)])\n",
    "        EZ = E * Z[0]\n",
    "        \n",
    "        for pos in range(self.W):\n",
    "            for i in range(self.n):\n",
    "                c_jk[pos][self.I[i][pos] - 1] += EZ[i]\n",
    "        \n",
    "        # start updating f\n",
    "        # won't directly update self.f since it will use the previous values to check the disparity\n",
    "        c_0k /= np.sum(c_0k, axis = 1)\n",
    "        for pos in range(self.W):\n",
    "            c_jk[pos] /= sum(c_jk[pos])\n",
    "        f_ij = np.vstack((c_0k,c_jk))\n",
    "        \n",
    "        self.update_erasing()\n",
    "        \n",
    "        return lamb, f_ij\n",
    "    \n",
    "    def update_variables(self, lamb, f_ij):\n",
    "        '''\n",
    "        update the variables' values after M step\n",
    "        it will update: f_ij, lambda, conditional probabilities p_X1 and p_X2, erasing\n",
    "        \n",
    "        Inputs:\n",
    "        lamb: lambda, size: 1 x 2\n",
    "        f_ij: the updated values for position-wise letter frequencies, size: (W+1) x L\n",
    "        '''\n",
    "        \n",
    "        self.lambda1 = lamb[0]\n",
    "        self.lambda2 = lamb[1]\n",
    "        self.f_ij = f_ij.copy()\n",
    "        self.p_X1, self.p_X2 = self.condi_distribution(self.f_ij)\n",
    "        self.update_erasing()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def iter(self, epsilon = 1e-6):\n",
    "        '''\n",
    "        iterate the E and M steps\n",
    "        '''\n",
    "        \n",
    "        err_ls = list()\n",
    "        f_ls = list()\n",
    "        f_ls.append(self.f_ij)\n",
    "        \n",
    "        for step in range(100):\n",
    "            condi_dis = np.array([self.p_X1, self.p_X2])\n",
    "            lamb = np.array([self.lambda1, self.lambda2])\n",
    "            new_Z = self.E_step(condi_dis, lamb)\n",
    "            new_lambda, new_f_ij = self.M_step()\n",
    "            f_ls.append(new_f_ij)\n",
    "            \n",
    "            err = np.sqrt(np.sum( (self.f_ij - new_f_ij) ** 2  , axis = 1))\n",
    "            err_ls.append(err)\n",
    "            \n",
    "            if any(err < epsilon):\n",
    "                #print('At step {}, EM converges to a solution.'.format(step))\n",
    "                #print('err is ', err)\n",
    "                self.update_variables(new_lambda, new_f_ij)\n",
    "                break\n",
    "            \n",
    "            '''if step % 10 == 0 and step > 0:\n",
    "                print('in step 10n err is ', err)\n",
    "            '''\n",
    "            self.update_variables(new_lambda, new_f_ij)\n",
    "        \n",
    "        return #err_ls, new_f_ij, f_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, model, data_in, W):\n",
    "        \n",
    "        self.model = model\n",
    "        self.data = data_in\n",
    "        self.W = W\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb98840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    data_info = {amount of sequence, }\n",
    "    '''\n",
    "    # read data\n",
    "    data_info\n",
    "    # build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dim(data, IO):\n",
    "    '''\n",
    "    To expand input and output data into specific dimensions\n",
    "    '''\n",
    "\n",
    "    row, col = data.size()\n",
    "    assert row < 50 and col < 500, 'Index out of range.'\n",
    "\n",
    "    if IO:# expand input to 50 x 500\n",
    "        col_expand = 7 * torch.ones(row, 500 - col)\n",
    "        row_expand = 7 * torch.ones(50 - row, 500)\n",
    "\n",
    "    elif not IO:# expand output to 50 x 50\n",
    "        col_expand = 7 * torch.ones(row, 50 - col)\n",
    "        row_expand = 7 * torch.ones(50 - row, 50)\n",
    "\n",
    "    res = torch.cat( (torch.cat((data, col_expand), axis = 1), row_expand), axis = 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_padding_mask(data):\n",
    "    mask = torch.zeros(data.size())\n",
    "    mask[data == 7] = -torch.inf\n",
    "    return mask\n",
    "\n",
    "tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['CAGTCAGCAC','AGTTACGTAG']\n",
    "data1 = ['AAAAA','AAAAA']\n",
    "m = MEME(data1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c21ba898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(m.f_ij, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "031d81d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.f_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb55e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
