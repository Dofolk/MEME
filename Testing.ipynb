{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u4lcFKDzPKgl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
        "    TransformerDecoder, TransformerDecoderLayer\n",
        "from torch.nn.functional import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lQGEaihkPKgq"
      },
      "outputs": [],
      "source": [
        "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}\n",
        "data = ['ATCGACTACG','CTGACTGAT']\n",
        "src = Tensor(list(map(lambda x: alphabet_dict[x], data[0]))).reshape(1,-1).long()\n",
        "tgt = Tensor(list(map(lambda x: alphabet_dict[x], data[1]))).reshape(1,-1).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xU60g3F8PKgr"
      },
      "outputs": [],
      "source": [
        "ntokens = 10\n",
        "d_model = 512\n",
        "dropout = 0.1\n",
        "max_len = 1000\n",
        "nhead = 8\n",
        "encoder_layer_nums = 6\n",
        "decoder_layer_nums = 6\n",
        "dim_ff = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IlsV6EghPKgs"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module): #done\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
        "        output = self.dropout(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xFvtPFIyPKgt"
      },
      "outputs": [],
      "source": [
        "emb = nn.Embedding(ntokens, d_model)\n",
        "pos_encode = PositionalEmbedding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA91_xV0PKgv"
      },
      "outputs": [],
      "source": [
        "decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout)\n",
        "memory = torch.rand(10, 32, 128)\n",
        "tgt = torch.rand(9, 32, 128)\n",
        "out = decoder_layer(tgt, memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# type 1\n",
        "transformer1 = nn.Transformer(d_model, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, dropout)"
      ],
      "metadata": {
        "id": "TkCIjgnDPiPV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type 2\n",
        "encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout,\n",
        "                    batch_first = True)\n",
        "encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
        "decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout,\n",
        "                    batch_first = True)\n",
        "decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n"
      ],
      "metadata": {
        "id": "VqlXYcPfPhSX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je7GDdBdPKgz",
        "outputId": "1bf00d6a-c04c-462e-e0ca-ed96d3d1e517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src's shape is torch.Size([2, 11])\n",
            "tgt's shape is torch.Size([2, 9])\n"
          ]
        }
      ],
      "source": [
        "# test data\n",
        "src = torch.LongTensor([\n",
        "    [0,8,3,5,5,9,6,1,2,2,2],\n",
        "    [0,6,6,8,9,1,2,2,2,2,2]\n",
        "])\n",
        "\n",
        "tgt = torch.LongTensor([\n",
        "    [0,8,3,5,5,9,6,1,2],\n",
        "    [0,6,6,8,9,1,2,2,2],\n",
        "])\n",
        "\n",
        "print('src\\'s shape is {}\\ntgt\\'s shape is {}'\\\n",
        "      .format(src.shape, tgt.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tt0Qsl83PKg0"
      },
      "outputs": [],
      "source": [
        "def get_pad(tokens):\n",
        "    key_padding_mask = torch.zeros(tokens.size())\n",
        "    key_padding_mask[tokens == 2] = -torch.inf\n",
        "    return key_padding_mask\n",
        "\n",
        "src_key_padding_mask = get_pad(src)\n",
        "tgt_key_padding_mask = get_pad(tgt)\n",
        "\n",
        "tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))\n",
        "\n",
        "transformer = nn.Transformer(d_model, batch_first = True)\n",
        "\n",
        "res = transformer(emb(src),emb(tgt),\n",
        "          tgt_mask = tgt_mask,\n",
        "          src_key_padding_mask = src_key_padding_mask,\n",
        "          tgt_key_padding_mask = tgt_key_padding_mask\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout,\n",
        "                    batch_first = True)\n",
        "encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
        "decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout,\n",
        "                    batch_first = True)\n",
        "decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)"
      ],
      "metadata": {
        "id": "9-_1WTP5jVgw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_Ndj05tLPKg0"
      },
      "outputs": [],
      "source": [
        "encode_out = encoder(emb(src))\n",
        "output = decoder(emb(tgt), encode_out, tgt_mask = tgt_mask, tgt_key_padding_mask = tgt_key_padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gDwKOeakPKg1"
      },
      "outputs": [],
      "source": [
        "output = decoder(pos_encode(emb(tgt)), pos_encode(emb(src)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.randint(10, (50, 100))\n",
        "tgt = torch.randint(10, (50, 50))\n",
        "tgt_key_mask = torch.zeros(tgt.size())\n",
        "for i in range(tgt.size(0)):\n",
        "  tgt_key_mask[i][3+i:] = -torch.inf\n",
        "\n",
        "tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))"
      ],
      "metadata": {
        "id": "yC6sGCtYUPlW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_dim(data, IO):\n",
        "  '''\n",
        "  To expand input and output data into specific dimensions\n",
        "  '''\n",
        "\n",
        "  row, col = data.size()\n",
        "  assert row < 50 and col < 500, 'Index out of range.'\n",
        "\n",
        "  if IO:# expand input to 50 x 500\n",
        "    col_expand = 7 * torch.ones(row, 500 - col)\n",
        "    row_expand = 7 * torch.ones(50 - row, 500)\n",
        "\n",
        "  elif not IO:# expand output to 50 x 50\n",
        "    col_expand = 7 * torch.ones(row, 50 - col)\n",
        "    row_expand = 7 * torch.ones(50 - row, 50)\n",
        "\n",
        "  res = torch.cat( (torch.cat((data, col_expand), axis = 1), row_expand), axis = 0)\n",
        "\n",
        "  return res\n",
        "\n",
        "def get_padding_mask(data):\n",
        "  mask = torch.zeros(data.size())\n",
        "  mask[data == 7] = -torch.inf\n",
        "  return mask\n",
        "\n",
        "tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))\n",
        "\n"
      ],
      "metadata": {
        "id": "X10wqILCWdqP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = torch.randint(10,(20,30))\n",
        "res = expand_dim(d, 0)\n",
        "print(res.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJW2wyF4Wd8v",
        "outputId": "af7723a3-d9a0-43ba-ab3a-29ae8de237d3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = torch.arange(6).reshape(2,3)\n",
        "m2 = torch.arange(6,12).reshape(2,3)"
      ],
      "metadata": {
        "id": "R5NOsybYWeZq"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat( (m1,torch.cat((m1,m2), axis = 1)),axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdjc4wKrdzQI",
        "outputId": "f1e77227-0e17-4d2a-fd07-7ac5edb2e8cc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  0,  1,  2,  6,  7,  8],\n",
              "        [ 3,  4,  5,  3,  4,  5,  9, 10, 11]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Spaiv1IGoGN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}