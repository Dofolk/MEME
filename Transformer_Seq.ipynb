{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The model is designed to analyze the consensus sequences in DNA sequences\n",
    "\n",
    "The input of this model will be in the 2D matrix:    \n",
    "    dimension: n x 1000, n is the amount of the input samples\n",
    "\n",
    "The output of the model will be in the 3D matrix:\n",
    "    dimension: n x 1000 x 4, n is the amoutn of the input samples\n",
    "               the meaning of the output is the position-wise appearance for each alphabets(ATCG)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac6800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
    "    TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn.functional import softmax\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc9d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global variables\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}\n",
    "\n",
    "combi_letters = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4, \n",
    "                 'AT' : 5, 'AC': 6, 'AG' : 7, 'TC' : 8, 'TG' : 9, \n",
    "                 'CG' : 10, 'ATC' : 11, 'ATG' : 12, 'ACG' : 13,\n",
    "                 'TCG' : 14, 'ATCG' : 15}\n",
    "val_to_char = {1 : 'A', 2 : 'T', 3 : 'C', 4 : 'G'}\n",
    "\n",
    "fileDir = r'D:\\motif data'\n",
    "fileExt = r'.fa'\n",
    "file_list = [f for f in os.listdir(fileDir) if f.endswith(fileExt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the fasta files and make the list of DNA sequences\n",
    "def read_file(path):\n",
    "    f = open(path,'r')\n",
    "    seq = {}\n",
    "\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            name = line.replace('>', '').split()[0]\n",
    "            seq[name] = ''\n",
    "        else:\n",
    "            seq[name] += line.replace('\\n','').strip()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    seq_ls = []\n",
    "    for name in seq:\n",
    "        seq_ls.append(seq[name])\n",
    "    return seq_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1833835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 500\n",
    "nhead = 8\n",
    "encoder_layer_nums = 6\n",
    "decoder_layer_nums = 6\n",
    "dim_ff = 512\n",
    "# token: ATCG(1~4), <BOS or SOS>(0), <EOS>(16), <PAD>(17)\n",
    "    # combinations: AT, AC, AG, TC, TG, CG, ATC, ATG, ACG, TCG, ATCG(5~15, total 11)\n",
    "ntoken = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8495e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #done\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        encoder_layer_nums,\n",
    "        decoder_layer_nums,\n",
    "        dim_ff,\n",
    "        ntoken,\n",
    "    ):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pos_encode = PositionalEmbedding(d_model, dropout, max_len)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n",
    "        self.src_embedding = nn.Embedding(ntoken, d_model, scale_grad_by_freq = False)\n",
    "        self.tgt_embedding = nn.Embedding(4, d_model, scale_grad_by_freq = False)\n",
    "        self.out_embed = nn.Embedding(ntoken, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, ntoken)\n",
    "        self.output_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask = None,\n",
    "        tgt_mask = None,\n",
    "        memory_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None,\n",
    "        memory_key_padding_mask = None,\n",
    "    ):\n",
    "        \n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        \n",
    "        src = self.pos_encode(src)\n",
    "        tgt = self.pos_encode(tgt)\n",
    "        \n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, \n",
    "                              tgt_mask = tgt_mask, \n",
    "                              memory_mask = memory_mask, \n",
    "                              tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask = memory_key_padding_mask\n",
    "                             )\n",
    "        output = self.output_linear(output)\n",
    "        output = self.output_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): #done\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51eb9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seq(input_seq, IO, pad_num = 17):\n",
    "    '''\n",
    "    adjust the length for single sequence\n",
    "    tokenize the sequence\n",
    "    return:\n",
    "    res: result for the single sequence adjustment and return in ndarray type\n",
    "    '''\n",
    "    \n",
    "    L = len(input_seq)\n",
    "    \n",
    "    if IO:\n",
    "        if L > 498:\n",
    "            mid = L//2\n",
    "            seq = input_seq[mid - 249: mid + 249]\n",
    "            res = np.array(list(map(lambda x: combi_letters[x], seq)))\n",
    "            res = np.concatenate((np.array([0]), res, np.array[16]))\n",
    "        else:\n",
    "            res = np.array(list(map(lambda x: combi_letters[x], input_seq)))\n",
    "            res = np.concatenate((np.array([0]), res, np.array([16]), np.array([pad_num] * (498 - L) )))\n",
    "        assert len(res) == 500, 'Preprocess the input sequence not equal to 500'\n",
    "    \n",
    "    else:\n",
    "        assert L <= 48, 'Preprocess the output sequence get the length error, > 48'\n",
    "        res = np.array(list(map(lambda x: combi_letters[x], input_seq)))\n",
    "        res = np.concatenate((np.array([0]), res, np.array([16]), np.array([pad_num] * (48 - L))))\n",
    "    \n",
    "    return res.astype('int')\n",
    "\n",
    "def make_outputs(data, row_size):\n",
    "    '''\n",
    "    make the outputs for the model, i.e. make the tgt\n",
    "    return:\n",
    "    tgt: the target array, has transfered the string data into tokens, np.array\n",
    "    '''\n",
    "    tgt = list()\n",
    "    l = list()\n",
    "    for seq in data:\n",
    "        l.append(len(seq))\n",
    "    assert min(l) > 3, 'The minimum length of the inputs are less than 3'\n",
    "    if min(l) < row_size:\n",
    "        row_size = min(l)\n",
    "        print(\"Mention: the target size is greater than the minimum length of the data, it will set the size as the minimum length of the data\")\n",
    "\n",
    "    for motif_len in range(row_size - 3):\n",
    "        #print('At step ', motif_len)\n",
    "        #print(f'making the length {motif_len+3}\\'s MEME output')\n",
    "        m = MEME(data, motif_len + 3)\n",
    "        start_time = time.time()\n",
    "        m.iter()\n",
    "        print(f'making the length {motif_len+3}\\'s MEME output use the time {time.time()-start_time}')\n",
    "        motif, logo = m.prob_out()\n",
    "        tgt.append(preprocess_seq(logo, 0))\n",
    "    tgt = np.array(tgt)\n",
    "    assert len(tgt.shape) == 2, f'tgt dimension error in make outputs and the shape is {tgt.shape}'\n",
    "    return tgt\n",
    "\n",
    "def expand_dim(data, IO, pad_num = 17, row_size = 40):\n",
    "    '''\n",
    "    To expand input and output data into specific dimensions\n",
    "    The input variable of this function is transfered matrix, that is the matrix with tokenized elements\n",
    "    return:\n",
    "    res: the results of expanding the dimension\n",
    "    '''\n",
    "\n",
    "    row, col = data.shape\n",
    "    assert row < row_size, 'Row size out of range.'\n",
    "\n",
    "    if IO:# expand input to row_size x 500\n",
    "        assert col <= 500, 'Column size out of range(Input)'\n",
    "        col_expand = pad_num * np.ones((row, 500 - col))\n",
    "        row_expand = pad_num * np.ones((row_size - row, 500))\n",
    "\n",
    "    elif not IO:# expand output to row_size x 50\n",
    "        assert col <= 50, 'Column size out of range(Output)'\n",
    "        col_expand = pad_num * np.ones((row, 50 - col))\n",
    "        row_expand = pad_num * np.ones((row_size - row, 50))\n",
    "\n",
    "    res = np.concatenate( (np.concatenate((data, col_expand), axis = 1), row_expand), axis = 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_padding_mask(data, pad_num = 17):\n",
    "    '''\n",
    "    create the padding mask for data\n",
    "    return:\n",
    "    mask: the padding mask in Tensor, with size same as the input data for the function\n",
    "    '''\n",
    "    mask = torch.zeros(data.shape)\n",
    "    mask[data == pad_num] = -torch.inf\n",
    "    \n",
    "    return mask\n",
    "\n",
    "#tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55a4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5,6]\n",
    "])\n",
    "\n",
    "tgt = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5]    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365d9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = TransformerModel(d_model, dropout, max_len, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, ntoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a24c627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 18])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl(src, tgt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea75b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613cd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1,0, gamma = 0.8)\n",
    "\n",
    "def get_batch(batch_size, ith):\n",
    "    \n",
    "    dna_seq = read_file(os.path.join(fileDir, file_list[ith]))\n",
    "    assert len(dna_seq) < batch_size, f' The (row) size of the input data is out of range(>{batch_size}).'\n",
    "    row = len(dna_seq)\n",
    "    target = make_outputs(dna_seq, batch_size)\n",
    "    data = np.zeros((row, 500))\n",
    "    for idx in range(len(dna_seq)):\n",
    "        print(f'doing the {idx} step in for loop')\n",
    "        data[idx] = preprocess_seq(dna_seq[idx], 1)\n",
    "        \n",
    "    data = expand_dim(data, 1)\n",
    "    return data, target\n",
    "\n",
    "def save_data():\n",
    "    '''\n",
    "    save the preprocessed data\n",
    "    since the \n",
    "    '''\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(1):\n",
    "        data, target = get_batch(40, step)\n",
    "        data_key_padding_mask = get_padding_mask(data)\n",
    "        target_key_padding_mask = get_padding_mask(target)\n",
    "        target_mask = nn.Transformer.generate_square_subsequent_mask(target.size(-1))\n",
    "        output = model(data, target, \n",
    "                       src_key_padding_mask = data_key_padding_mask,\n",
    "                       tgt_key_padding_mask = target_key_padding_mask,\n",
    "                       tgt_mask = target_mask\n",
    "                      )\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, target.veiw(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step% 20 == 0:\n",
    "            print(f'Step {step}, total loss: {total_loss}')\n",
    "            total_loss = 0\n",
    "    \n",
    "    return\n",
    "\n",
    "def evaluate(model, eval_data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(1):\n",
    "            data, targets = get_batch()\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntoken)\n",
    "            total_loss += seq_len * criterion(output_flat, targets.view(-1)).item()\n",
    "        \n",
    "    return total_loss / (len(eval_data) - 1)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f54618a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dna_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m MEME(dna_seq, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m m\u001b[38;5;241m.\u001b[39miter()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dna_seq' is not defined"
     ]
    }
   ],
   "source": [
    "m = MEME(dna_seq, 20)\n",
    "start_time = time.time()\n",
    "m.iter()\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cb98840",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mention: the target size is greater than the minimum length of the data, it will set the size as the minimum length of the data\n",
      "making the length 3's MEME output use the time 10.256240367889404\n",
      "making the length 4's MEME output use the time 11.786333799362183\n",
      "making the length 5's MEME output use the time 1.0089032649993896\n",
      "making the length 6's MEME output use the time 14.54584813117981\n",
      "making the length 7's MEME output use the time 15.272658824920654\n",
      "making the length 8's MEME output use the time 0.17101764678955078\n",
      "making the length 9's MEME output use the time 17.363539934158325\n",
      "making the length 10's MEME output use the time 18.07251811027527\n",
      "making the length 11's MEME output use the time 0.16188907623291016\n",
      "making the length 12's MEME output use the time 0.10732913017272949\n",
      "making the length 13's MEME output use the time 0.2673914432525635\n",
      "making the length 14's MEME output use the time 0.19844818115234375\n",
      "making the length 15's MEME output use the time 0.3223719596862793\n",
      "making the length 16's MEME output use the time 0.15200471878051758\n",
      "making the length 17's MEME output use the time 0.18983864784240723\n",
      "making the length 18's MEME output use the time 19.69049882888794\n",
      "making the length 19's MEME output use the time 19.729743719100952\n",
      "making the length 20's MEME output use the time 0.18889331817626953\n",
      "making the length 21's MEME output use the time 0.18840885162353516\n",
      "making the length 22's MEME output use the time 0.15564751625061035\n",
      "making the length 23's MEME output use the time 0.1508796215057373\n",
      "making the length 24's MEME output use the time 0.12392663955688477\n",
      "making the length 25's MEME output use the time 0.10045075416564941\n",
      "making the length 26's MEME output use the time 0.0766592025756836\n",
      "making the length 27's MEME output use the time 0.0508880615234375\n",
      "making the length 28's MEME output use the time 0.05843997001647949\n",
      "making the length 29's MEME output use the time 0.05680966377258301\n",
      "making the length 30's MEME output use the time 0.0475771427154541\n",
      "doing the 0 step in for loop\n",
      "doing the 1 step in for loop\n",
      "doing the 2 step in for loop\n",
      "doing the 3 step in for loop\n",
      "doing the 4 step in for loop\n",
      "doing the 5 step in for loop\n",
      "doing the 6 step in for loop\n",
      "doing the 7 step in for loop\n",
      "doing the 8 step in for loop\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m mdl \u001b[38;5;241m=\u001b[39m TransformerModel(d_model, dropout, max_len, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, ntoken)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#data = read_file(path)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# build model and train\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train(mdl)\n",
      "Cell \u001b[1;32mIn[67], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     32\u001b[0m data, target \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;241m40\u001b[39m, step)\n\u001b[0;32m     33\u001b[0m data_key_padding_mask \u001b[38;5;241m=\u001b[39m get_padding_mask(data)\n\u001b[1;32m---> 34\u001b[0m target_key_padding_mask \u001b[38;5;241m=\u001b[39m get_padding_mask(target)\n\u001b[0;32m     35\u001b[0m target_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     36\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data, target, \n\u001b[0;32m     37\u001b[0m                src_key_padding_mask \u001b[38;5;241m=\u001b[39m data_key_padding_mask,\n\u001b[0;32m     38\u001b[0m                tgt_key_padding_mask \u001b[38;5;241m=\u001b[39m target_key_padding_mask,\n\u001b[0;32m     39\u001b[0m                tgt_mask \u001b[38;5;241m=\u001b[39m target_mask\n\u001b[0;32m     40\u001b[0m               )\n",
      "Cell \u001b[1;32mIn[71], line 90\u001b[0m, in \u001b[0;36mget_padding_mask\u001b[1;34m(data, pad_num)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03mcreate the padding mask for data\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mreturn:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mmask: the padding mask in Tensor, with size same as the input data for the function\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     89\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 90\u001b[0m mask[data \u001b[38;5;241m==\u001b[39m pad_num] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    mdl = TransformerModel(d_model, dropout, max_len, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, ntoken)\n",
    "    \n",
    "    # read data\n",
    "    #data = read_file(path)\n",
    "    # build model and train\n",
    "    \n",
    "    train(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "510ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['CAGTCAGCAC','AGTTACGTAG']\n",
    "data1 = ['AAAAA','AAAAA']\n",
    "m = MEME(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "478f1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = TransformerModel(d_model, dropout, max_len, nhead,\n",
    "                       encoder_layer_nums, decoder_layer_nums,\n",
    "                       dim_ff, ntoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f420781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encode): PositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(18, 512)\n",
       "  (out_embed): Embedding(18, 512)\n",
       "  (output_linear): Linear(in_features=512, out_features=18, bias=True)\n",
       "  (output_softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5,6]\n",
    "])\n",
    "\n",
    "tgt = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5]    \n",
    "])\n",
    "\n",
    "mdl(src,tgt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = mdl.embedding\n",
    "mdl.decoder(emb(tgt),emb(src)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8152373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]),\n",
       " ['T', 'T', 'C', 'G'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=['TTCGATGCCGT','CTTAATGCTAC']\n",
    "m=MEME(data,4)\n",
    "m.iter()\n",
    "m.prob_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4303d567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]),\n",
       " ['A', 'T', 'C', 'G'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a905f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TGCTGGATAAGAATGTTTTAGCAATCTCTTT', 'TCAGCGAAAAAAATTAAAGCGCAAGATTGTT', 'GCGACAACCGGAATATGAAAGCAAAGCGCAG', 'TGCTGGATAAGAATGTTTTAGCAATCTCTTT', 'TCAGCGAAAAAAATTAAAGCGCAAGATTGTT', 'GCGACAACCGGAATATGAAAGCAAAGCGCAG', 'TGCTGGATAAGAATGTTTTAGCAATCTCTTT', 'TCAGCGAAAAAAATTAAAGCGCAAGATTGTT', 'GCGACAACCGGAATATGAAAGCAAAGCGCAG']\n"
     ]
    }
   ],
   "source": [
    "dna_seq = read_file(os.path.join(fileDir, file_list[0]))\n",
    "print(dna_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0acf8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3960171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MEME(dna_seq, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e61f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.269115686416626\n"
     ]
    }
   ],
   "source": [
    "stt = time.time()\n",
    "record = m.iter()\n",
    "print(time.time() - stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ecd6e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.010507583618164062, 0.0065097808837890625, 0.0, 0.010503530502319336, 0.006506919860839844, 0.0, 0.009590625762939453, 0.004101276397705078, 0.0, 0.006003856658935547, 0.002595186233520508, 0.0, 0.006006479263305664, 0.0025844573974609375, 0.0, 0.0059604644775390625, 0.002000093460083008, 0.0, 0.00650477409362793, 0.0029997825622558594, 0.0, 0.005504608154296875, 0.0030002593994140625, 0.0, 0.005504131317138672, 0.0035047531127929688, 0.0, 0.004999876022338867, 0.0032253265380859375, 0.0, 0.00599980354309082, 0.002000570297241211, 0.0, 0.00621485710144043, 0.003000020980834961, 0.0, 0.006071567535400391, 0.003000020980834961, 0.0, 0.005505561828613281, 0.0029997825622558594, 0.0, 0.005125999450683594, 0.003000020980834961, 0.0, 0.00599980354309082, 0.0031998157501220703, 0.0, 0.008270740509033203, 0.00599980354309082, 0.0, 0.010504484176635742, 0.0060002803802490234, 0.0009999275207519531, 0.010010242462158203, 0.006038188934326172, 0.0, 0.010000228881835938, 0.00599980354309082, 0.0, 0.010679244995117188, 0.0057239532470703125, 0.0, 0.01039743423461914, 0.00600433349609375, 0.0, 0.009507417678833008, 0.0069997310638427734, 0.0, 0.010509967803955078, 0.006512641906738281, 0.0, 0.011018037796020508, 0.005505084991455078, 0.0, 0.01051187515258789, 0.0060002803802490234, 0.0, 0.010511159896850586, 0.006513118743896484, 0.0, 0.010511398315429688, 0.005999565124511719, 0.0, 0.011512041091918945, 0.005513668060302734, 0.0, 0.010666131973266602, 0.007126569747924805, 0.0, 0.010183572769165039, 0.006001710891723633, 0.0, 0.010512351989746094, 0.006576061248779297, 0.0, 0.0105743408203125, 0.0055048465728759766, 0.0, 0.010564804077148438, 0.0060727596282958984, 0.0, 0.010125160217285156, 0.0056383609771728516, 0.0, 0.01062464714050293, 0.005941867828369141, 0.0, 0.011014223098754883, 0.005508899688720703, 0.0, 0.01000070571899414, 0.005998849868774414, 0.0, 0.010509252548217773, 0.0065081119537353516, 0.0, 0.010506868362426758, 0.005503416061401367, 0.0010042190551757812, 0.009890079498291016, 0.005508899688720703, 0.0, 0.009999752044677734, 0.006690502166748047, 0.0, 0.010008573532104492, 0.006002664566040039, 0.0, 0.011274337768554688, 0.005504131317138672, 0.0, 0.010000228881835938, 0.007504701614379883, 0.0, 0.009503841400146484, 0.0060002803802490234, 0.0, 0.01050710678100586, 0.0060002803802490234, 0.0, 0.009504318237304688, 0.007002115249633789, 0.0, 0.010145425796508789, 0.0055065155029296875, 0.0, 0.011087894439697266, 0.006005525588989258, 0.0, 0.010506391525268555, 0.005505800247192383, 0.0, 0.010577917098999023, 0.005966663360595703, 0.0, 0.010507822036743164, 0.005504131317138672, 0.0, 0.011004209518432617, 0.005506992340087891, 0.0, 0.010999679565429688, 0.005506038665771484, 0.0, 0.010150671005249023, 0.006511211395263672, 0.0, 0.009507417678833008, 0.006506681442260742, 0.0, 0.010507583618164062, 0.0059223175048828125, 0.0, 0.010506391525268555, 0.006506443023681641, 0.0, 0.00959920883178711, 0.00651097297668457, 0.0, 0.00962376594543457, 0.006699323654174805, 0.0, 0.010337114334106445, 0.00528717041015625, 0.0, 0.010637998580932617, 0.0039365291595458984, 0.0, 0.004999637603759766, 0.0035545825958251953, 0.0, 0.004952669143676758, 0.0029997825622558594, 0.0, 0.005507707595825195, 0.003000020980834961, 0.0, 0.005507469177246094, 0.003000020980834961, 0.0, 0.004587411880493164, 0.0035064220428466797, 0.0, 0.00604248046875, 0.002538919448852539, 0.0, 0.00604248046875, 0.0029611587524414062, 0.0, 0.005543947219848633, 0.0024623870849609375, 0.0, 0.005505800247192383, 0.0030448436737060547, 0.0, 0.006554365158081055, 0.007001399993896484, 0.0, 0.011080265045166016, 0.006593227386474609, 0.0, 0.010002374649047852, 0.005786895751953125, 0.0008084774017333984, 0.00945734977722168, 0.006003141403198242, 0.0, 0.009999990463256836, 0.006504058837890625, 0.0, 0.009504318237304688, 0.0065042972564697266, 0.0, 0.009999990463256836, 0.006504535675048828, 0.0, 0.010504722595214844, 0.006000041961669922, 0.0, 0.010504484176635742, 0.0060002803802490234, 0.0, 0.010504007339477539, 0.0060002803802490234, 0.0010001659393310547, 0.009504079818725586, 0.006000041961669922, 0.0, 0.010504722595214844, 0.006098508834838867, 0.0, 0.01006937026977539, 0.0065042972564697266, 0.0, 0.010107040405273438, 0.006506204605102539, 0.0010004043579101562, 0.009999752044677734, 0.0055713653564453125, 0.0009982585906982422, 0.009602546691894531, 0.005967617034912109, 0.0, 0.010538816452026367, 0.005545139312744141, 0.0, 0.007057905197143555, 0.003003358840942383, 0.0, 0.005269289016723633, 0.0030014514923095703, 0.0, 0.0055103302001953125, 0.003000020980834961, 0.0, 0.005504608154296875, 0.003003835678100586, 0.0, 0.005000114440917969, 0.0035076141357421875, 0.0, 0.0060689449310302734, 0.002710580825805664, 0.0, 0.0049288272857666016, 0.002993345260620117, 0.0, 0.00555109977722168, 0.00299835205078125, 0.0, 0.005637407302856445, 0.0030024051666259766, 0.0, 0.005537271499633789, 0.003002643585205078, 0.0, 0.0050008296966552734, 0.0035033226013183594, 0.0, 0.0050585269927978516, 0.0029976367950439453, 0.0, 0.005548715591430664, 0.0029985904693603516, 0.0, 0.005504608154296875, 0.0029997825622558594, 0.0, 0.005504608154296875, 0.003000020980834961, 0.0, 0.0065021514892578125, 0.0020024776458740234, 0.0, 0.006000041961669922, 0.0025022029876708984, 0.0, 0.0060024261474609375, 0.0029997825622558594, 0.0, 0.00550532341003418, 0.004999399185180664, 0.0, 0.010312080383300781, 0.0061855316162109375, 0.0, 0.01050567626953125, 0.006000041961669922, 0.0, 0.009504556655883789, 0.006501674652099609, 0.0, 0.010286569595336914, 0.0058650970458984375, 0.0, 0.01050424575805664, 0.00599980354309082, 0.0, 0.009504318237304688, 0.0065042972564697266, 0.0, 0.009999990463256836, 0.00599980354309082, 0.0, 0.01050424575805664, 0.00599980354309082, 0.0, 0.010294198989868164, 0.005638599395751953, 0.0, 0.010616302490234375, 0.005562543869018555, 0.0, 0.01006174087524414, 0.00671839714050293, 0.0, 0.008853435516357422, 0.006623029708862305, 0.0, 0.010452508926391602, 0.0055027008056640625, 0.0, 0.01079249382019043, 0.0055081844329833984, 0.0, 0.010508298873901367, 0.005997896194458008, 0.0, 0.010571718215942383, 0.005724430084228516, 0.0, 0.010458707809448242, 0.006004810333251953, 0.0, 0.009506940841674805, 0.0070018768310546875, 0.0, 0.009500503540039062, 0.0062139034271240234, 0.0, 0.010602951049804688, 0.0059468746185302734, 0.0, 0.010137796401977539, 0.006504535675048828, 0.0, 0.010008573532104492, 0.006000041961669922, 0.0, 0.010130643844604492, 0.0060002803802490234, 0.0, 0.009504318237304688, 0.006999969482421875, 0.0, 0.009926319122314453, 0.005578279495239258, 0.0, 0.010504484176635742, 0.00599980354309082, 0.0, 0.010505199432373047, 0.005999565124511719, 0.0, 0.00950479507446289, 0.0065042972564697266, 0.0, 0.010235786437988281, 0.006000041961669922, 0.0, 0.009504079818725586, 0.006000041961669922, 0.0, 0.01050424575805664, 0.0065021514892578125, 0.0, 0.010002374649047852, 0.0065042972564697266, 0.0, 0.009999990463256836, 0.0055043697357177734, 0.0, 0.010000228881835938, 0.0065038204193115234, 0.0, 0.009999990463256836, 0.005504131317138672, 0.0, 0.01100015640258789, 0.005504131317138672, 0.0, 0.01100015640258789, 0.005504608154296875, 0.0, 0.010201454162597656, 0.006000041961669922, 0.0010004043579101562, 0.009502410888671875, 0.006002664566040039, 0.0, 0.010504007339477539, 0.006502389907836914, 0.0, 0.010506629943847656, 0.006000041961669922, 0.0, 0.010088682174682617, 0.00599980354309082, 0.0, 0.010618209838867188, 0.005886077880859375, 0.0, 0.010504722595214844, 0.00650477409362793, 0.0, 0.00950479507446289, 0.005999565124511719, 0.0, 0.010504722595214844, 0.005999565124511719, 0.0, 0.011067390441894531, 0.00599980354309082, 0.0, 0.009505271911621094, 0.006000041961669922, 0.0, 0.01050424575805664, 0.006505012512207031, 0.0, 0.01050710678100586, 0.00599980354309082, 0.0, 0.009504556655883789, 0.006504535675048828, 0.0, 0.009999752044677734, 0.006504058837890625, 0.0, 0.010570526123046875, 0.00599980354309082, 0.0, 0.010162830352783203, 0.005504608154296875, 0.0, 0.009999752044677734, 0.0065042972564697266, 0.0, 0.010000228881835938, 0.006577730178833008, 0.0, 0.010428905487060547, 0.006003141403198242, 0.0, 0.010076761245727539, 0.00500035285949707, 0.0, 0.011009454727172852, 0.006000041961669922, 0.0, 0.010071039199829102, 0.0065042972564697266, 0.0, 0.010504722595214844, 0.006000041961669922, 0.0, 0.010504484176635742, 0.00599980354309082, 0.0, 0.010504484176635742, 0.006050586700439453, 0.0, 0.010212421417236328, 0.00691533088684082, 0.0, 0.010008811950683594, 0.00599980354309082, 0.0, 0.010504961013793945, 0.00650477409362793, 0.0, 0.010632753372192383, 0.006003379821777344, 0.0, 0.009560346603393555, 0.00650477409362793, 0.0, 0.010504007339477539, 0.005602121353149414, 0.0, 0.01050710678100586, 0.006058454513549805, 0.0, 0.010617494583129883, 0.006506204605102539, 0.0, 0.010036468505859375, 0.006539106369018555, 0.0, 0.010030269622802734, 0.006155729293823242, 0.0, 0.010041236877441406, 0.0060062408447265625, 0.0, 0.01060342788696289, 0.006627082824707031, 0.0, 0.010005712509155273, 0.006056308746337891, 0.0, 0.010200738906860352, 0.0056915283203125, 0.0, 0.010942459106445312, 0.0050563812255859375, 0.0, 0.01050877571105957, 0.006508350372314453, 0.0, 0.010000228881835938, 0.00599980354309082, 0.0009999275207519531, 0.009507179260253906, 0.0060002803802490234, 0.0, 0.010509252548217773, 0.006422758102416992, 0.0, 0.009511232376098633, 0.006507396697998047, 0.0, 0.01050877571105957, 0.005507469177246094, 0.0, 0.010000228881835938, 0.006506919860839844, 0.0, 0.010321617126464844, 0.006000518798828125, 0.0, 0.009508371353149414, 0.0055027008056640625, 0.0, 0.010006189346313477, 0.006500959396362305, 0.0, 0.009823083877563477, 0.0055119991302490234, 0.0009996891021728516, 0.009999990463256836, 0.006506681442260742, 0.0, 0.010510683059692383, 0.005503416061401367, 0.0, 0.010539054870605469, 0.005506277084350586, 0.0, 0.009633541107177734, 0.006003618240356445, 0.0, 0.010565757751464844, 0.006176948547363281, 0.0, 0.010106563568115234, 0.005999565124511719, 0.0, 0.010199785232543945, 0.006186008453369141, 0.0, 0.009566545486450195, 0.006089210510253906, 0.0, 0.01063227653503418, 0.006005525588989258, 0.0, 0.009506464004516602, 0.006506919860839844, 0.0, 0.010506868362426758, 0.00599980354309082, 0.0, 0.009697198867797852, 0.006974220275878906, 0.0, 0.009150981903076172, 0.007002592086791992, 0.0, 0.009469985961914062, 0.005999565124511719, 0.0, 0.010508537292480469, 0.0055084228515625, 0.0, 0.010000228881835938, 0.00599980354309082, 0.0, 0.010506153106689453, 0.0057680606842041016, 0.0, 0.005042076110839844, 0.0029973983764648438, 0.0, 0.0055866241455078125, 0.003004312515258789, 0.0, 0.005593299865722656, 0.0030422210693359375, 0.0, 0.005517005920410156, 0.0029485225677490234, 0.0, 0.005000114440917969, 0.003504037857055664, 0.0, 0.005000114440917969, 0.003000020980834961, 0.0, 0.005504131317138672, 0.0030002593994140625, 0.0, 0.005504131317138672, 0.003000020980834961, 0.0, 0.005501747131347656, 0.0020024776458740234, 0.0, 0.009504556655883789, 0.006000041961669922, 0.0, 0.009505510330200195, 0.006505012512207031, 0.0, 0.01050567626953125, 0.006504535675048828, 0.0, 0.010505437850952148, 0.0061206817626953125, 0.0, 0.009504556655883789, 0.006000041961669922, 0.0, 0.01050424575805664, 0.00599980354309082, 0.0, 0.0101318359375, 0.006504058837890625, 0.0, 0.010504484176635742, 0.00599980354309082, 0.0, 0.009504318237304688, 0.00599980354309082, 0.0, 0.011008977890014648, 0.005682706832885742, 0.0, 0.009691715240478516, 0.0069427490234375, 0.0, 0.009685754776000977, 0.005941629409790039, 0.0, 0.010509729385375977, 0.006130218505859375, 0.0, 0.010159969329833984, 0.006999492645263672, 0.0, 0.00957345962524414, 0.006509065628051758, 0.0, 0.010508060455322266, 0.0055084228515625, 0.0, 0.010746955871582031, 0.0059986114501953125, 0.0, 0.01050567626953125, 0.006545066833496094, 0.0, 0.010468482971191406, 0.006001472473144531, 0.0, 0.010506629943847656, 0.005000591278076172, 0.0015032291412353516, 0.009003877639770508, 0.006506204605102539, 0.0, 0.010505914688110352, 0.0055768489837646484, 0.0, 0.0059926509857177734, 0.002997159957885742, 0.0, 0.005630016326904297, 0.0029997825622558594, 0.0, 0.0046346187591552734, 0.0029947757720947266, 0.0, 0.00500035285949707, 0.003508329391479492, 0.0, 0.00599980354309082, 0.002553701400756836, 0.0, 0.007953166961669922, 0.006506443023681641, 0.0, 0.009510040283203125, 0.0060002803802490234, 0.0, 0.009506940841674805, 0.0055065155029296875, 0.0, 0.010506391525268555, 0.0065059661865234375, 0.0, 0.010607242584228516, 0.00600433349609375, 0.0, 0.009633541107177734, 0.007005453109741211, 0.0, 0.009508132934570312, 0.006508827209472656, 0.0, 0.010509014129638672, 0.00599980354309082, 0.0, 0.0096282958984375, 0.0069997310638427734, 0.0, 0.010130167007446289, 0.0056896209716796875, 0.0, 0.010706901550292969, 0.005873441696166992, 0.0, 0.010508537292480469, 0.0059986114501953125, 0.0, 0.00950765609741211, 0.006506443023681641, 0.0, 0.010508298873901367, 0.005999565124511719, 0.0, 0.009999990463256836, 0.006506919860839844, 0.0, 0.009506940841674805, 0.006508827209472656, 0.0, 0.009507417678833008, 0.006000041961669922, 0.0, 0.010507345199584961, 0.006506681442260742, 0.0, 0.009623289108276367, 0.0059435367584228516, 0.0, 0.010507583618164062, 0.005999565124511719, 0.0, 0.01050710678100586, 0.005507707595825195, 0.0, 0.010512113571166992, 0.005999565124511719, 0.0, 0.010085344314575195, 0.006506443023681641, 0.0, 0.010000467300415039, 0.005506753921508789, 0.0, 0.01056051254272461, 0.005948781967163086, 0.0, 0.01050877571105957, 0.005509376525878906, 0.0, 0.010506153106689453, 0.006002902984619141, 0.0, 0.010506629943847656, 0.005511045455932617, 0.0, 0.010507822036743164, 0.0065042972564697266, 0.0, 0.009003639221191406, 0.007203578948974609, 0.0, 0.009622812271118164, 0.007006168365478516, 0.0, 0.009505033493041992, 0.006000041961669922, 0.0, 0.010715723037719727, 0.00607752799987793, 0.0, 0.0096282958984375, 0.006507158279418945, 0.0, 0.010631322860717773, 0.006003141403198242, 0.0, 0.010223627090454102, 0.006506919860839844, 0.0, 0.010000467300415039, 0.006577968597412109, 0.0, 0.010497808456420898, 0.006000995635986328, 0.0, 0.010509967803955078, 0.005537748336791992, 0.0, 0.010558128356933594, 0.005563974380493164, 0.0010037422180175781, 0.009472131729125977, 0.0069997310638427734, 0.0, 0.0095062255859375, 0.007000446319580078, 0.0, 0.0095062255859375, 0.006777286529541016, 0.0, 0.009696722030639648, 0.005995512008666992, 0.0, 0.011148691177368164, 0.0060596466064453125, 0.0, 0.010570764541625977, 0.0056722164154052734, 0.0, 0.010540485382080078, 0.006000041961669922, 0.0, 0.010505914688110352, 0.0060007572174072266, 0.0, 0.009572029113769531, 0.006000041961669922, 0.0, 0.010642766952514648, 0.006005525588989258, 0.0, 0.010396003723144531, 0.007604122161865234, 0.0, 0.010518074035644531, 0.0055027008056640625, 0.0010042190551757812, 0.00950765609741211, 0.005999565124511719, 0.0, 0.010506868362426758, 0.006506443023681641, 0.0, 0.009999990463256836, 0.004999876022338867, 0.0, 0.010506629943847656, 0.006506443023681641, 0.0, 0.010222673416137695, 0.006000041961669922, 0.0010001659393310547, 0.009564876556396484, 0.00555729866027832, 0.0, 0.011248350143432617, 0.006229400634765625, 0.0, 0.010234832763671875, 0.006085395812988281, 0.0, 0.009507417678833008, 0.00600433349609375, 0.0, 0.010510444641113281, 0.005507707595825195, 0.0009999275207519531, 0.00950765609741211, 0.005999565124511719, 0.0, 0.009510517120361328, 0.003223419189453125, 0.0, 0.006044626235961914, 0.0025968551635742188, 0.0, 0.0055999755859375, 0.002996683120727539, 0.0, 0.0055882930755615234, 0.002959012985229492, 0.0, 0.005561113357543945, 0.002943277359008789, 0.0, 0.006000041961669922, 0.0025043487548828125, 0.0, 0.004999637603759766, 0.003000497817993164, 0.0, 0.00550389289855957, 0.003000020980834961, 0.0, 0.005504131317138672, 0.003000020980834961, 0.0, 0.005555152893066406, 0.002948760986328125, 0.0, 0.00500035285949707, 0.003504037857055664, 0.0, 0.004999876022338867, 0.0030002593994140625, 0.0, 0.0062103271484375, 0.0019998550415039062, 0.0009999275207519531, 0.005000114440917969, 0.002504110336303711, 0.0, 0.006000041961669922, 0.0030596256256103516, 0.0, 0.004999876022338867, 0.003000020980834961, 0.0, 0.0052106380462646484, 0.0029997825622558594, 0.0, 0.006000041961669922, 0.002000093460083008, 0.0, 0.006000041961669922, 0.003000020980834961, 0.0, 0.0055043697357177734, 0.003000020980834961, 0.0, 0.0055043697357177734, 0.002000093460083008, 0.0, 0.00599980354309082, 0.003504037857055664, 0.0, 0.00500035285949707, 0.0019998550415039062, 0.0, 0.0065042972564697266, 0.003000020980834961, 0.0, 0.005504131317138672, 0.0029997825622558594, 0.0, 0.005504608154296875, 0.0025060176849365234, 0.0, 0.00599980354309082, 0.0030379295349121094, 0.0, 0.005000114440917969, 0.00350189208984375, 0.0, 0.005002260208129883, 0.0030002593994140625, 0.0, 0.005504608154296875, 0.003000020980834961, 0.0, 0.0055043697357177734, 0.0029997825622558594, 0.0, 0.005000114440917969, 0.0035042762756347656, 0.0, 0.005000114440917969, 0.002000093460083008, 0.0, 0.006504058837890625, 0.0030002593994140625, 0.0, 0.005504131317138672, 0.003000020980834961, 0.0, 0.005504131317138672, 0.003000020980834961, 0.0, 0.005000114440917969, 0.0035042762756347656, 0.0, 0.004501819610595703, 0.0030024051666259766, 0.0, 0.006210803985595703, 0.0030002593994140625, 0.0, 0.005053520202636719, 0.0029997825622558594, 0.0, 0.005311012268066406, 0.0025043487548828125, 0.0, 0.004999876022338867, 0.0035042762756347656, 0.0, 0.006000518798828125, 0.0025014877319335938, 0.0, 0.0060024261474609375, 0.0029997825622558594, 0.0, 0.0055043697357177734, 0.002000093460083008, 0.0, 0.005504608154296875, 0.003000020980834961, 0.0, 0.004999876022338867, 0.003504037857055664, 0.0, 0.006143331527709961, 0.002557992935180664, 0.0, 0.0060024261474609375, 0.0019998550415039062, 0.0, 0.006052494049072266, 0.002956867218017578, 0.0, 0.006095170974731445, 0.0030066967010498047, 0.0, 0.005585908889770508, 0.00295257568359375, 0.0, 0.003999948501586914, 0.003504514694213867, 0.0, 0.0060002803802490234, 0.0025022029876708984, 0.0, 0.006002187728881836, 0.003000497817993164, 0.0, 0.0070798397064208984, 0.006504535675048828, 0.0, 0.009501934051513672, 0.0060024261474609375, 0.0, 0.009999990463256836, 0.0065953731536865234, 0.0, 0.010505914688110352, 0.0060002803802490234, 0.0, 0.010603666305541992, 0.00590062141418457, 0.0, 0.010504484176635742, 0.0055043697357177734, 0.0, 0.010000228881835938, 0.006247758865356445, 0.0, 0.010504722595214844, 0.0061664581298828125, 0.0, 0.009691476821899414, 0.00656437873840332, 0.0, 0.010064125061035156, 0.005778074264526367, 0.0, 0.010300636291503906, 0.0060045719146728516, 0.0, 0.010506153106689453, 0.00599980354309082, 0.0, 0.009506940841674805, 0.006537437438964844, 0.0, 0.010581493377685547, 0.006005048751831055, 0.0, 0.009823322296142578, 0.0069353580474853516, 0.0, 0.009507417678833008, 0.006581306457519531, 0.0, 0.010109901428222656, 0.005621194839477539, 0.0, 0.010005712509155273, 0.006505012512207031, 0.0, 0.010508537292480469, 0.0059986114501953125, 0.0, 0.010506629943847656, 0.005511045455932617, 0.0, 0.01050710678100586, 0.00599980354309082, 0.0, 0.010509252548217773, 0.006506443023681641, 0.0, 0.009724855422973633, 0.006717205047607422, 0.0, 0.010067462921142578, 0.005636692047119141, 0.0, 0.01063084602355957, 0.005562782287597656, 0.0010628700256347656, 0.009941339492797852, 0.005507469177246094, 0.0, 0.010506868362426758, 0.0060002803802490234, 0.0, 0.010506868362426758, 0.006011962890625, 0.001004934310913086, 0.009508609771728516, 0.006077766418457031, 0.0, 0.010425090789794922, 0.006005764007568359, 0.0, 0.00950479507446289, 0.006505012512207031, 0.0, 0.010506868362426758, 0.006061553955078125, 0.0, 0.010450124740600586, 0.00599980354309082, 0.0, 0.009506940841674805, 0.00599980354309082, 0.0, 0.010055065155029297, 0.006506919860839844, 0.0009999275207519531, 0.010659217834472656, 0.0058481693267822266, 0.0, 0.010508537292480469, 0.005505561828613281, 0.0, 0.010999679565429688, 0.005502939224243164, 0.0, 0.010004520416259766, 0.005999326705932617, 0.0, 0.010511398315429688, 0.006502866744995117, 0.0, 0.010512351989746094, 0.00602412223815918, 0.0, 0.01050710678100586, 0.005504131317138672, 0.0010051727294921875, 0.009505510330200195, 0.0060040950775146484, 0.0, 0.009999990463256836, 0.006661891937255859, 0.0, 0.01050567626953125, 0.006503105163574219, 0.0, 0.010002613067626953, 0.0055043697357177734, 0.0, 0.010505199432373047, 0.006505727767944336, 0.0, 0.009504079818725586, 0.006000041961669922, 0.0, 0.010605096817016602, 0.005900382995605469, 0.0, 0.01050424575805664, 0.005713224411010742, 0.0, 0.010504722595214844, 0.00599980354309082, 0.0, 0.010506629943847656, 0.0060002803802490234, 0.0, 0.010009288787841797, 0.006502628326416016, 0.0, 0.010002613067626953, 0.006504058837890625, 0.0, 0.009504556655883789, 0.006501913070678711, 0.0, 0.010002374649047852, 0.006504535675048828, 0.0, 0.010000228881835938, 0.005999565124511719, 0.0, 0.010504484176635742, 0.005504608154296875, 0.0, 0.010505199432373047, 0.00599980354309082, 0.0, 0.010069131851196289, 0.005504131317138672, 0.0009999275207519531, 0.009140729904174805, 0.006636381149291992, 0.0, 0.00957036018371582, 0.006003856658935547, 0.0, 0.01100015640258789, 0.005728006362915039, 0.0, 0.010281801223754883, 0.006198406219482422, 0.0, 0.010312080383300781, 0.00574493408203125, 0.0, 0.010523080825805664, 0.005753517150878906, 0.0, 0.010509729385375977, 0.005507946014404297, 0.0, 0.00999903678894043, 0.0072062015533447266, 0.0, 0.009505748748779297, 0.006506443023681641, 0.0, 0.01050710678100586, 0.006002902984619141, 0.0, 0.009634971618652344, 0.006330966949462891, 0.0, 0.010584831237792969, 0.005966663360595703, 0.0, 0.00969696044921875, 0.0068094730377197266, 0.0, 0.009507179260253906, 0.006000518798828125, 0.0, 0.010506153106689453, 0.006000041961669922, 0.0, 0.010579824447631836, 0.005927562713623047, 0.0, 0.010506629943847656, 0.005502939224243164, 0.0, 0.010511636734008789, 0.00599980354309082, 0.0, 0.010506391525268555, 0.0060002803802490234, 0.0, 0.009725093841552734, 0.006685733795166016, 0.0, 0.010000944137573242, 0.005568504333496094, 0.0, 0.011513471603393555, 0.006999969482421875, 0.0, 0.010508298873901367, 0.006010532379150391, 0.0, 0.010003328323364258, 0.006505727767944336, 0.0, 0.010061025619506836, 0.0065648555755615234, 0.0, 0.009617090225219727, 0.006006717681884766, 0.0, 0.010505199432373047, 0.005534172058105469, 0.0010006427764892578, 0.009508609771728516, 0.007002592086791992, 0.0, 0.009504079818725586, 0.006001472473144531, 0.0, 0.010014533996582031, 0.006061077117919922, 0.0, 0.010570764541625977, 0.006000041961669922, 0.0, 0.010506868362426758, 0.006000041961669922, 0.0, 0.00970458984375, 0.005805015563964844, 0.0, 0.010504484176635742, 0.006510496139526367, 0.0, 0.009507179260253906, 0.006000041961669922, 0.0, 0.010180950164794922, 0.0060002803802490234, 0.0, 0.010509252548217773, 0.006508350372314453, 0.0, 0.009503841400146484, 0.006004810333251953, 0.0, 0.010506629943847656, 0.006000518798828125, 0.0, 0.010260581970214844, 0.0055086612701416016, 0.0, 0.010000228881835938, 0.00599980354309082, 0.0, 0.011013269424438477, 0.005506277084350586, 0.0, 0.011000633239746094, 0.0066988468170166016, 0.0, 0.011442422866821289, 0.0063054561614990234, 0.0, 0.010319948196411133, 0.0060040950775146484, 0.0, 0.010697364807128906, 0.005942106246948242, 0.0, 0.005627632141113281, 0.0029375553131103516, 0.0, 0.005507469177246094, 0.002001047134399414, 0.0, 0.005999088287353516, 0.0025076866149902344, 0.0, 0.006119728088378906, 0.005627632141113281, 0.0, 0.01007986068725586, 0.005869150161743164, 0.0, 0.010504722595214844, 0.00650334358215332, 0.0, 0.010002374649047852, 0.006000041961669922, 0.0, 0.01050424575805664, 0.0055043697357177734, 0.0, 0.009999990463256836, 0.006504535675048828, 0.0, 0.010504484176635742, 0.004999876022338867, 0.0, 0.010504484176635742, 0.0065038204193115234, 0.0, 0.010000228881835938, 0.005501985549926758, 0.0, 0.011002302169799805, 0.0055043697357177734, 0.0, 0.010000228881835938, 0.0065042972564697266, 0.0, 0.010999917984008789, 0.0055048465728759766, 0.0, 0.010504961013793945, 0.006505489349365234, 0.0, 0.009504556655883789, 0.00599980354309082, 0.0009999275207519531, 0.008999824523925781, 0.006601095199584961, 0.0, 0.010782003402709961, 0.00562596321105957, 0.0, 0.010504961013793945, 0.0060002803802490234, 0.0, 0.009504079818725586, 0.00599980354309082, 0.0, 0.010009765625, 0.00599980354309082, 0.0, 0.010664224624633789, 0.0058400630950927734, 0.0, 0.010505437850952148, 0.00599980354309082, 0.0, 0.010274410247802734, 0.006230354309082031, 0.0, 0.009614944458007812, 0.005889892578125, 0.0, 0.010504961013793945, 0.00599980354309082, 0.0, 0.009504318237304688, 0.006504535675048828, 0.0, 0.009505510330200195, 0.00599980354309082, 0.0, 0.01050424575805664, 0.00599980354309082, 0.0, 0.01050424575805664, 0.006000041961669922, 0.0, 0.009504556655883789, 0.0065042972564697266, 0.0, 0.009999752044677734, 0.006504535675048828, 0.0, 0.009999752044677734, 0.005505084991455078, 0.0, 0.009999752044677734, 0.0065042972564697266, 0.0, 0.009999990463256836, 0.006561756134033203, 0.0, 0.010444402694702148, 0.0060024261474609375, 0.0, 0.009999990463256836, 0.0055043697357177734, 0.0, 0.009999990463256836, 0.006504058837890625, 0.0, 0.010000944137573242, 0.0070078372955322266, 0.0, 0.009508371353149414, 0.0060002803802490234, 0.0010001659393310547, 0.009504079818725586, 0.006000041961669922, 0.0, 0.009501934051513672, 0.006526470184326172, 0.0, 0.010504961013793945, 0.0060002803802490234, 0.0, 0.009503841400146484, 0.0070002079010009766, 0.0, 0.010234355926513672, 0.005518674850463867, 0.0, 0.010008811950683594, 0.0060002803802490234, 0.0, 0.010504484176635742, 0.006504058837890625, 0.0, 0.009504079818725586, 0.007000446319580078, 0.0, 0.009606122970581055, 0.005898237228393555, 0.0, 0.010504722595214844, 0.006000041961669922, 0.0, 0.009505987167358398, 0.00599980354309082, 0.0, 0.011009931564331055, 0.005999565124511719, 0.0, 0.009508609771728516, 0.006504058837890625, 0.0, 0.010506391525268555, 0.00599980354309082, 0.0, 0.010000228881835938, 0.00599980354309082, 0.0, 0.010505199432373047, 0.0055048465728759766, 0.0, 0.010774850845336914, 0.005504131317138672, 0.0, 0.010505199432373047, 0.005505084991455078, 0.0, 0.01100015640258789, 0.005504131317138672, 0.0, 0.009999990463256836, 0.0065042972564697266, 0.0, 0.010504007339477539, 0.006088733673095703, 0.0, 0.010009050369262695, 0.0060002803802490234, 0.0, 0.009504318237304688, 0.006002187728881836, 0.0010001659393310547, 0.00970768928527832, 0.006628513336181641, 0.0, 0.009621381759643555, 0.006571769714355469, 0.0, 0.00950932502746582, 0.005999326705932617, 0.0010030269622802734, 0.009892940521240234, 0.005611896514892578, 0.0, 0.010507583618164062, 0.00599980354309082, 0.0, 0.010506391525268555, 0.006000518798828125, 0.0, 0.009507417678833008, 0.006507396697998047, 0.0, 0.010157585144042969, 0.0055048465728759766, 0.0, 0.010506153106689453, 0.006057262420654297, 0.0, 0.01050710678100586, 0.0060617923736572266, 0.0, 0.009640932083129883, 0.006931304931640625, 0.0, 0.009564638137817383, 0.0060062408447265625, 0.0009999275207519531, 0.009631633758544922, 0.006557464599609375, 0.0, 0.010062932968139648, 0.005795955657958984, 0.0, 0.010338783264160156, 0.0060045719146728516, 0.0, 0.01050710678100586, 0.005507946014404297, 0.0, 0.010698080062866211, 0.0055081844329833984, 0.0, 0.010507583618164062, 0.005999326705932617, 0.0, 0.010568618774414062, 0.005445003509521484, 0.0, 0.01051640510559082, 0.0055084228515625, 0.0009996891021728516, 0.009506940841674805, 0.00599980354309082, 0.0, 0.009999752044677734, 0.006567955017089844, 0.0, 0.010570764541625977, 0.005876779556274414, 0.0, 0.01050877571105957, 0.0055084228515625, 0.0, 0.010506629943847656, 0.0060002803802490234, 0.0, 0.009829521179199219, 0.00700068473815918, 0.0, 0.009507179260253906, 0.006000995635986328, 0.0, 0.010505914688110352, 0.005506992340087891, 0.0, 0.010506629943847656, 0.006202220916748047, 0.0, 0.009506702423095703, 0.005505561828613281, 0.0, 0.009508132934570312, 0.006000041961669922, 0.0, 0.010009765625, 0.0060002803802490234, 0.0, 0.010504722595214844, 0.00599980354309082, 0.0, 0.010505437850952148, 0.00599980354309082, 0.0, 0.009504556655883789, 0.006999969482421875, 0.0, 0.010506153106689453, 0.006502866744995117, 0.0, 0.01100015640258789, 0.0055048465728759766, 0.0, 0.01050424575805664, 0.006018638610839844, 0.0, 0.010505199432373047, 0.005999326705932617, 0.0, 0.010504722595214844, 0.0055048465728759766, 0.0, 0.01050257682800293, 0.006002902984619141, 0.0, 0.009999990463256836, 0.0055043697357177734, 0.0, 0.009999990463256836, 0.006691455841064453, 0.0, 0.010316848754882812, 0.0055997371673583984, 0.0, 0.01040959358215332, 0.006002664566040039, 0.0, 0.010504484176635742, 0.00599980354309082, 0.0, 0.010504484176635742, 0.00599980354309082, 0.0, 0.009504556655883789, 0.006000041961669922, 0.0, 0.010504484176635742, 0.006000041961669922, 0.0, 0.009505271911621094, 0.006505250930786133, 0.0, 0.010505437850952148, 0.0060002803802490234, 0.0, 0.010504961013793945, 0.005501985549926758, 0.0, 0.010002374649047852, 0.0065042972564697266, 0.0, 0.010504722595214844, 0.006000041961669922, 0.0, 0.010504007339477539, 0.00599980354309082, 0.0, 0.010506391525268555, 0.0060198307037353516, 0.0, 0.010509490966796875, 0.005998849868774414, 0.0, 0.009185314178466797, 0.0025877952575683594, 0.0, 0.006045341491699219, 0.002003908157348633, 0.0, 0.006590366363525391, 0.0029997825622558594, 0.0, 0.005461931228637695, 0.0019998550415039062, 0.0, 0.006505012512207031, 0.003000020980834961, 0.0, 0.008504390716552734, 0.00599980354309082, 0.0, 0.009504556655883789, 0.0065042972564697266, 0.0, 0.010504007339477539, 0.005505561828613281, 0.0, 0.010066747665405273, 0.00599980354309082, 0.0, 0.009504079818725586, 0.006000041961669922, 0.0, 0.01050424575805664, 0.006501913070678711, 0.0, 0.009002208709716797, 0.006504535675048828, 0.0, 0.009999752044677734, 0.0065042972564697266, 0.0, 0.010170459747314453, 0.006247758865356445, 0.0, 0.010131597518920898, 0.006000041961669922, 0.0, 0.010196447372436523, 0.006506204605102539, 0.0, 0.009829044342041016, 0.005856990814208984, 0.0, 0.010571956634521484, 0.0055065155029296875, 0.0, 0.01050567626953125, 0.006063699722290039, 0.0, 0.010447978973388672, 0.0055086612701416016, 0.0, 0.010508060455322266, 0.005507707595825195, 0.0, 0.01007223129272461, 0.0070040225982666016, 0.0, 0.010509014129638672, 0.005506753921508789, 0.0, 0.010506868362426758, 0.00599980354309082, 0.0, 0.0101318359375, 0.006003618240356445, 0.0, 0.010462045669555664, 0.006009817123413086, 0.0, 0.010286092758178711, 0.006636857986450195, 0.0, 0.010595560073852539, 0.005715847015380859, 0.0, 0.010512113571166992, 0.006003618240356445, 0.0, 0.008746862411499023, 0.0030498504638671875, 0.0, 0.005605936050415039, 0.003004312515258789, 0.0, 0.009451866149902344, 0.006002187728881836, 0.0, 0.009647846221923828, 0.005568742752075195, 0.0, 0.010626554489135742, 0.006608009338378906, 0.0, 0.010469436645507812, 0.006132841110229492, 0.0, 0.008934259414672852, 0.0060350894927978516, 0.0, 0.010564088821411133, 0.006506443023681641, 0.0, 0.009603738784790039, 0.0029685497283935547, 0.0, 0.004994869232177734, 0.003643035888671875, 0.0, 0.004998445510864258, 0.0025627613067626953, 0.0, 0.005658626556396484, 0.003000497817993164, 0.0, 0.005571842193603516, 0.0030002593994140625, 0.0, 0.0055065155029296875, 0.003000020980834961, 0.0, 0.004999876022338867, 0.0035076141357421875, 0.0, 0.00599980354309082, 0.0025582313537597656, 0.0, 0.004951000213623047, 0.0029976367950439453, 0.0, 0.005507230758666992, 0.0029997825622558594, 0.0, 0.008507966995239258, 0.0060002803802490234, 0.0, 0.01050710678100586, 0.006506919860839844, 0.0, 0.010009050369262695, 0.0067768096923828125, 0.0, 0.010364532470703125, 0.006003856658935547, 0.0, 0.01050567626953125, 0.005701541900634766, 0.0, 0.010307550430297852, 0.005980730056762695, 0.0, 0.0107269287109375, 0.0060577392578125, 0.0, 0.010200023651123047, 0.005566120147705078, 0.0010001659393310547, 0.009580612182617188, 0.006596565246582031, 0.0, 0.008539915084838867, 0.0060045719146728516, 0.0, 0.00950932502746582, 0.006508827209472656, 0.0, 0.010694265365600586, 0.006059408187866211, 0.0, 0.009507179260253906, 0.005000114440917969, 0.0015032291412353516, 0.009952306747436523, 0.0055086612701416016, 0.0, 0.010508060455322266, 0.006008625030517578, 0.0, 0.010005712509155273, 0.006505250930786133, 0.0, 0.009508609771728516, 0.004999876022338867, 0.0, 0.010507345199584961, 0.00599980354309082, 0.0, 0.010507345199584961, 0.006506204605102539, 0.0, 0.009538888931274414, 0.006000041961669922, 0.0, 0.01047825813293457, 0.00599980354309082, 0.0, 0.009624958038330078, 0.006165504455566406, 0.0, 0.010723352432250977, 0.00599980354309082, 0.0, 0.009505987167358398, 0.006506443023681641, 0.0, 0.010066747665405273, 0.006617069244384766, 0.0, 0.009627342224121094, 0.003055572509765625, 0.0, 0.011454343795776367, 0.0066373348236083984, 0.0, 0.009969949722290039, 0.0060002803802490234, 0.0, 0.010504007339477539, 0.006098270416259766, 0.0, 0.009703397750854492, 0.006933689117431641, 0.0, 0.00962519645690918, 0.005998849868774414, 0.0010006427764892578, 0.009632110595703125, 0.006563663482666016, 0.0, 0.009633541107177734, 0.00599980354309082, 0.0, 0.0108184814453125, 0.005872964859008789, 0.0, 0.009566545486450195, 0.0061037540435791016, 0.0, 0.009507179260253906, 0.006066799163818359, 0.0, 0.010441303253173828, 0.00599980354309082, 0.0, 0.010509729385375977, 0.005505800247192383, 0.0, 0.010506153106689453, 0.005999326705932617, 0.0, 0.010015010833740234, 0.0059986114501953125, 0.0, 0.010759115219116211, 0.005997896194458008, 0.0, 0.010545730590820312, 0.0055065155029296875, 0.0, 0.010030508041381836, 0.00654149055480957, 0.0, 0.010506391525268555, 0.0054738521575927734, 0.0, 0.010567903518676758, 0.005995035171508789, 0.0, 0.010208368301391602, 0.005631208419799805, 0.0, 0.010505914688110352, 0.00599980354309082, 0.0, 0.010082721710205078, 0.00650787353515625, 0.0, 0.012505769729614258, 0.005999565124511719, 0.0, 0.010576725006103516, 0.006000041961669922, 0.0, 0.01018214225769043, 0.005898475646972656, 0.0, 0.010017156600952148, 0.006509304046630859, 0.0, 0.009507179260253906, 0.006000041961669922, 0.0010001659393310547, 0.009506940841674805, 0.0060579776763916016, 0.0, 0.010633707046508789, 0.006994485855102539, 0.0, 0.010565757751464844, 0.0059986114501953125, 0.0, 0.010507345199584961, 0.005509614944458008, 0.0, 0.010507583618164062, 0.006012439727783203, 0.0, 0.010003328323364258, 0.006513118743896484, 0.0, 0.0095062255859375, 0.006502866744995117, 0.0, 0.010003805160522461, 0.00650787353515625, 0.0, 0.009686708450317383, 0.005820035934448242, 0.0009999275207519531, 0.010507583618164062, 0.006506919860839844, 0.0, 0.009999990463256836, 0.006016969680786133, 0.0, 0.010506629943847656, 0.005999088287353516, 0.0, 0.009508371353149414, 0.00599980354309082, 0.0, 0.011513948440551758, 0.0059986114501953125, 0.0, 0.010507822036743164, 0.005555391311645508, 0.0, 0.010587930679321289, 0.007993936538696289, 0.0, 0.009507417678833008, 0.006406068801879883, 0.0, 0.010574579238891602, 0.006003856658935547, 0.0, 0.010510444641113281, 0.005997419357299805, 0.0, 0.010609149932861328, 0.006220102310180664, 0.0, 0.010619640350341797, 0.00599980354309082, 0.0, 0.010610818862915039, 0.0065555572509765625, 0.0, 0.010009527206420898, 0.005507469177246094, 0.0, 0.011003255844116211, 0.005506992340087891, 0.0, 0.010512351989746094, 0.005999326705932617, 0.0, 0.009506940841674805, 0.00650787353515625, 0.0, 0.010656356811523438, 0.005997419357299805, 0.0, 0.009506940841674805, 0.006683826446533203, 0.0, 0.010541915893554688, 0.005549192428588867, 0.0, 0.010692119598388672, 0.006452798843383789, 0.0, 0.010172605514526367, 0.006332874298095703, 0.0, 0.009193897247314453, 0.007066249847412109, 0.0, 0.009626388549804688, 0.006443500518798828, 0.0, 0.010066986083984375, 0.006582021713256836, 0.0, 0.010120391845703125, 0.0055065155029296875, 0.0, 0.010508060455322266, 0.006508588790893555, 0.0, 0.010002374649047852, 0.00650477409362793, 0.0, 0.011745929718017578, 0.005998373031616211, 0.0, 0.0095062255859375, 0.0070002079010009766, 0.0, 0.010055065155029297, 0.005502223968505859, 0.0, 0.010002851486206055, 0.006009340286254883, 0.0, 0.009999990463256836, 0.006711721420288086, 0.0, 0.009629249572753906, 0.00699305534362793, 0.0, 0.010416030883789062, 0.006035804748535156, 0.0, 0.010508537292480469, 0.0061070919036865234, 0.0, 0.011017560958862305, 0.0055027008056640625, 0.0, 0.010004758834838867, 0.00650334358215332, 0.0, 0.01017904281616211, 0.007001399993896484, 0.0, 0.011504650115966797, 0.006502866744995117, 0.0, 0.010002851486206055, 0.00650477409362793, 0.0, 0.010161638259887695, 0.005856037139892578, 0.0, 0.010444402694702148, 0.005949258804321289, 0.0, 0.009635210037231445, 0.005950927734375, 0.0010042190551757812, 0.009999752044677734, 0.006509304046630859, 0.0, 0.010509252548217773, 0.006000041961669922, 0.0, 0.00950312614440918, 0.006003856658935547, 0.0, 0.01050710678100586, 0.006507396697998047, 0.0, 0.010000467300415039, 0.005507946014404297, 0.0, 0.012508630752563477, 0.006571531295776367, 0.0, 0.010071277618408203, 0.00600433349609375, 0.0, 0.009605884552001953, 0.006999969482421875, 0.0, 0.009566068649291992, 0.00655674934387207, 0.0, 0.009529829025268555, 0.0060346126556396484, 0.0, 0.010476827621459961, 0.00599980354309082, 0.0, 0.010509252548217773, 0.00599980354309082, 0.0, 0.011012792587280273, 0.00550532341003418, 0.0, 0.010063409805297852, 0.006936311721801758, 0.0, 0.010577201843261719, 0.0059926509857177734, 0.0, 0.012071371078491211, 0.006063222885131836, 0.0, 0.010449409484863281, 0.006062507629394531, 0.0, 0.010501623153686523, 0.005065202713012695, 0.0, 0.01126718521118164, 0.0059337615966796875, 0.0, 0.01061558723449707, 0.005967378616333008, 0.0, 0.010574817657470703, 0.006507158279418945, 0.0, 0.009692192077636719, 0.006937503814697266, 0.0, 0.010350942611694336, 0.00599980354309082, 0.0, 0.010088443756103516, 0.005938291549682617, 0.0, 0.005697965621948242, 0.0019974708557128906, 0.0010056495666503906, 0.010508298873901367, 0.006573915481567383, 0.0, 0.01044011116027832, 0.00614166259765625, 0.0, 0.008858442306518555, 0.00754237174987793, 0.0, 0.009706735610961914, 0.00683283805847168, 0.0, 0.009606599807739258, 0.005898952484130859, 0.0, 0.011008262634277344, 0.006609678268432617, 0.0, 0.010004043579101562, 0.004999876022338867, 0.0, 0.01116490364074707, 0.003001689910888672, 0.0, 0.005630016326904297, 0.003002166748046875, 0.0, 0.005627155303955078, 0.0030028820037841797, 0.0, 0.005564689636230469, 0.002074718475341797, 0.0, 0.005930185317993164, 0.005684852600097656, 0.0010004043579101562, 0.010564565658569336, 0.0070002079010009766, 0.0, 0.010076045989990234, 0.0055065155029296875, 0.0, 0.011062145233154297, 0.005753993988037109, 0.0, 0.010432243347167969, 0.006066799163818359, 0.0, 0.009280681610107422, 0.006506681442260742, 0.0, 0.011502981185913086, 0.0060350894927978516, 0.0, 0.01058053970336914, 0.006008148193359375, 0.0, 0.010141849517822266, 0.006036520004272461, 0.0010068416595458984, 0.009567022323608398, 0.005992889404296875, 0.0, 0.009541511535644531, 0.00700068473815918, 0.0, 0.010079145431518555, 0.006427288055419922, 0.0, 0.010506153106689453, 0.006029367446899414, 0.0, 0.010544300079345703, 0.00563359260559082, 0.0, 0.010933399200439453, 0.006078481674194336, 0.0, 0.00955820083618164, 0.005999326705932617, 0.0, 0.011081695556640625, 0.005999326705932617, 0.0, 0.009539365768432617, 0.006506204605102539, 0.0, 0.009999990463256836, 0.00599980354309082, 0.0, 0.010785341262817383, 0.00569605827331543, 0.0, 0.009996414184570312, 0.0061376094818115234, 0.0, 0.01000666618347168, 0.006507396697998047, 0.0, 0.010572671890258789, 0.005560159683227539, 0.0, 0.011060953140258789, 0.006051301956176758, 0.0, 0.01003122329711914, 0.006003618240356445, 0.0, 0.010000228881835938, 0.006506443023681641, 0.0, 0.010509490966796875, 0.006510019302368164, 0.0, 0.012359619140625, 0.005503654479980469, 0.0010044574737548828, 0.009502172470092773, 0.00606226921081543, 0.0, 0.01056981086730957, 0.006623268127441406, 0.0, 0.010517597198486328, 0.005997896194458008, 0.0, 0.010509014129638672, 0.005509138107299805, 0.0, 0.010057926177978516, 0.006636381149291992, 0.0, 0.0095672607421875, 0.0065076351165771484, 0.0, 0.009999752044677734, 0.006507396697998047, 0.0, 0.010000228881835938, 0.0055084228515625, 0.0, 0.011000394821166992, 0.00851130485534668, 0.0, 0.009507179260253906, 0.005998849868774414, 0.0, 0.010507583618164062, 0.006012916564941406, 0.0, 0.009508132934570312, 0.007005214691162109, 0.0, 0.009507894515991211, 0.0065076351165771484, 0.0, 0.009507179260253906, 0.0065114498138427734, 0.0, 0.01314544677734375, 0.005515098571777344, 0.0, 0.010564088821411133, 0.005941867828369141, 0.0, 0.010513067245483398, 0.0055043697357177734, 0.0, 0.009999752044677734, 0.00650787353515625, 0.0, 0.010000228881835938, 0.0055086612701416016, 0.0, 0.010595321655273438, 0.005511045455932617, 0.0, 0.011000394821166992, 0.004999637603759766, 0.0, 0.01050710678100586, 0.00650334358215332, 0.0, 0.01058650016784668, 0.005506038665771484, 0.0, 0.01101231575012207, 0.00600433349609375, 0.0, 0.009508371353149414, 0.00599980354309082, 0.0, 0.011000394821166992, 0.005506992340087891, 0.0, 0.01050710678100586, 0.006258964538574219, 0.0, 0.008741617202758789, 0.006506919860839844, 0.0, 0.010508060455322266, 0.00599980354309082, 0.0, 0.010018587112426758, 0.0060694217681884766, 0.0, 0.010550498962402344, 0.005889177322387695, 0.0, 0.010505914688110352, 0.00700068473815918, 0.0, 0.010781288146972656, 0.0065495967864990234, 0.0, 0.010630369186401367, 0.005873680114746094, 0.0, 0.010507583618164062, 0.005999565124511719, 0.0, 0.010010719299316406, 0.005999326705932617, 0.0, 0.010505437850952148, 0.0060002803802490234, 0.0, 0.010505437850952148, 0.00650477409362793, 0.0, 0.009999990463256836, 0.006747007369995117, 0.0, 0.010259628295898438, 0.006005525588989258, 0.0, 0.010507822036743164, 0.005506992340087891, 0.0, 0.010270118713378906, 0.0059967041015625, 0.0, 0.010000467300415039, 0.006640434265136719, 0.0, 0.009942054748535156, 0.005506992340087891, 0.0, 0.010507345199584961, 0.005999565124511719, 0.0, 0.010509014129638672, 0.0055692195892333984, 0.0009393692016601562, 0.009501457214355469, 0.00600433349609375, 0.0, 0.01001119613647461, 0.007006168365478516, 0.0, 0.010197162628173828, 0.006564617156982422, 0.0, 0.009454011917114258, 0.0060002803802490234, 0.0, 0.010470390319824219, 0.006003618240356445, 0.0, 0.010161876678466797, 0.006508588790893555, 0.0, 0.010509252548217773, 0.005998134613037109, 0.0, 0.010138273239135742, 0.006067514419555664, 0.0, 0.009566307067871094, 0.006002187728881836, 0.0, 0.009507894515991211, 0.006000041961669922, 0.0, 0.010507345199584961, 0.006506443023681641, 0.0, 0.010574102401733398, 0.0049343109130859375, 0.0, 0.010509252548217773, 0.006507396697998047, 0.0, 0.009999513626098633, 0.0060122013092041016, 0.0, 0.010000228881835938, 0.00686192512512207, 0.0, 0.010148286819458008, 0.006003618240356445, 0.0, 0.010507822036743164, 0.006506204605102539, 0.0, 0.009570121765136719, 0.005999565124511719, 0.0, 0.010000228881835938, 0.006989479064941406, 0.0, 0.010506868362426758, 0.006009817123413086, 0.0, 0.010506391525268555, 0.0060002803802490234, 0.0, 0.010689258575439453, 0.005941152572631836, 0.0, 0.010507345199584961, 0.007066965103149414, 0.0, 0.010579824447631836, 0.006997823715209961, 0.0, 0.010017156600952148, 0.0060002803802490234, 0.0, 0.009847879409790039, 0.007008790969848633, 0.0, 0.01100015640258789, 0.005506277084350586, 0.0010006427764892578, 0.009507894515991211, 0.005998849868774414, 0.0, 0.010508060455322266, 0.006505727767944336, 0.0, 0.010609149932861328, 0.005000114440917969, 0.0, 0.008995532989501953, 0.006620645523071289, 0.0, 0.010545969009399414, 0.006539583206176758, 0.0, 0.009999752044677734, 0.006506681442260742, 0.0, 0.010506391525268555, 0.0060002803802490234, 0.0, 0.00956583023071289, 0.006507158279418945, 0.0, 0.010507583618164062, 0.006999969482421875, 0.0, 0.009698152542114258, 0.0064394474029541016, 0.0, 0.010067939758300781, 0.00795888900756836, 0.0, 0.006063699722290039, 0.003651142120361328, 0.0009210109710693359, 0.009500265121459961, 0.006064653396606445, 0.0, 0.010809183120727539, 0.00578761100769043, 0.0, 0.010585546493530273, 0.006287813186645508, 0.0, 0.009998798370361328, 0.005506277084350586, 0.0, 0.010506153106689453, 0.0060002803802490234, 0.0, 0.011049747467041016, 0.005995273590087891, 0.0, 0.00957632064819336, 0.006582021713256836, 0.0, 0.01007223129272461, 0.0058896541595458984, 0.0, 0.010656595230102539, 0.006063938140869141, 0.0, 0.010504484176635742, 0.006000041961669922, 0.0, 0.0101318359375, 0.005504608154296875, 0.0, 0.010504722595214844, 0.00599980354309082, 0.0, 0.010504722595214844, 0.005501985549926758, 0.0, 0.010002374649047852, 0.00599980354309082, 0.0, 0.010504484176635742, 0.006197929382324219, 0.0, 0.010000228881835938, 0.006505250930786133, 0.0, 0.009812116622924805, 0.0072481632232666016, 0.0, 0.009747982025146484, 0.005995035171508789, 0.0, 0.010643720626831055, 0.005570888519287109, 0.0, 0.010606050491333008, 0.00665593147277832, 0.0, 0.010417461395263672, 0.0058405399322509766, 0.0, 0.00950765609741211, 0.006508588790893555, 0.0, 0.01051020622253418, 0.00599980354309082, 0.0, 0.009507179260253906, 0.0065724849700927734, 0.0, 0.009508371353149414, 0.0060002803802490234, 0.0, 0.007224082946777344, 0.0030014514923095703, 0.0, 0.0056018829345703125, 0.002995014190673828, 0.0, 0.004552125930786133, 0.0030410289764404297, 0.0, 0.00596165657043457, 0.0032172203063964844, 0.0, 0.007999897003173828, 0.006178140640258789, 0.0, 0.01050424575805664, 0.006000041961669922, 0.0, 0.009743928909301758, 0.006258726119995117, 0.0, 0.010009765625, 0.0065593719482421875, 0.0, 0.010661602020263672, 0.005999565124511719, 0.0, 0.010506391525268555, 0.0055043697357177734, 0.0, 0.011003494262695312, 0.005691051483154297, 0.0, 0.010508298873901367, 0.005814790725708008, 0.0, 0.010563373565673828, 0.005455732345581055, 0.0, 0.009999513626098633, 0.005506992340087891, 0.0, 0.009999990463256836, 0.006508350372314453, 0.0, 0.010000467300415039, 0.0065114498138427734, 0.0, 0.0105133056640625, 0.006646871566772461, 0.0, 0.010367155075073242, 0.006006956100463867, 0.00099945068359375, 0.010510683059692383, 0.0055119991302490234, 0.0, 0.010998010635375977, 0.006001710891723633, 0.0010006427764892578, 0.010509729385375977, 0.006514787673950195, 0.0, 0.010511398315429688, 0.0060579776763916016, 0.0, 0.0105133056640625, 0.006523847579956055, 0.0, 0.010428428649902344, 0.006005048751831055, 0.0, 0.00954747200012207, 0.006847381591796875, 0.0, 0.00963592529296875, 0.005933284759521484, 0.0, 0.010507345199584961, 0.006505727767944336, 0.0, 0.01037144660949707, 0.0056056976318359375, 0.0, 0.011537551879882812, 0.006654977798461914, 0.0, 0.010322809219360352, 0.006007194519042969, 0.0, 0.009502649307250977, 0.006510496139526367, 0.0, 0.010065793991088867, 0.005965232849121094, 0.0, 0.010574817657470703, 0.006155729293823242, 0.0, 0.010546207427978516, 0.0060002803802490234, 0.0, 0.009734630584716797, 0.006061553955078125, 0.0, 0.010880231857299805, 0.005621671676635742, 0.0, 0.011035919189453125, 0.0065670013427734375, 0.0, 0.010574579238891602, 0.005999088287353516, 0.0, 0.010509490966796875, 0.0065038204193115234, 0.0, 0.01000356674194336, 0.006506919860839844, 0.0, 0.009507179260253906, 0.006000041961669922, 0.0, 0.010507822036743164, 0.006503582000732422, 0.0, 0.01000356674194336, 0.006672382354736328, 0.0, 0.01019906997680664, 0.0060002803802490234, 0.0, 0.010508298873901367, 0.0055081844329833984, 0.0, 0.010000228881835938, 0.006507158279418945, 0.0, 0.01072549819946289, 0.005999088287353516, 0.0, 0.010117292404174805, 0.00603175163269043, 0.0, 0.009543418884277344, 0.006503582000732422, 0.0, 0.0076448917388916016, 0.003045320510864258, 0.0, 0.005537748336791992, 0.0020432472229003906, 0.0, 0.005959510803222656, 0.002592802047729492, 0.0, 0.009563207626342773, 0.006003141403198242, 0.0, 0.010762929916381836, 0.006008625030517578, 0.0, 0.01062464714050293, 0.005949258804321289, 0.0, 0.010508298873901367, 0.006581306457519531, 0.0, 0.009647846221923828, 0.006950855255126953, 0.0, 0.009504556655883789, 0.006999969482421875, 0.0, 0.009504556655883789, 0.006503105163574219, 0.0, 0.010002851486206055, 0.006505012512207031, 0.0, 0.010135650634765625, 0.006506204605102539, 0.0, 0.009000539779663086, 0.007506847381591797, 0.0, 0.009576559066772461, 0.006574869155883789, 0.0, 0.010003328323364258, 0.006506443023681641, 0.0, 0.010000228881835938, 0.0065081119537353516, 0.0, 0.010036230087280273, 0.0062389373779296875, 0.001001596450805664, 0.009514331817626953, 0.006578207015991211, 0.0, 0.009538888931274414, 0.006107330322265625, 0.0, 0.011133670806884766, 0.0060040950775146484, 0.0, 0.010039091110229492, 0.006003856658935547, 0.0, 0.010505199432373047, 0.006039619445800781, 0.0, 0.00957036018371582, 0.0061092376708984375, 0.0, 0.00962519645690918, 0.006005764007568359, 0.0, 0.010446548461914062, 0.006058216094970703, 0.0, 0.010636568069458008, 0.005553245544433594, 0.0, 0.009542703628540039, 0.0065081119537353516, 0.0, 0.010506868362426758, 0.0060002803802490234, 0.0, 0.01050710678100586, 0.005511760711669922]\n"
     ]
    }
   ],
   "source": [
    "print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e54da1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = np.array(record).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b66c3bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04378748, 9.54441357, 5.65681934])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rec, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb220629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2479"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b34c8c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.]]),\n",
       " ['A',\n",
       "  'G',\n",
       "  'C',\n",
       "  'G',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'T',\n",
       "  'T',\n",
       "  'A',\n",
       "  'A',\n",
       "  'A',\n",
       "  'G',\n",
       "  'C'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e356a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03984674, 0.02145594, 0.0164751 , 0.02222222]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.beta_i.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c97e88d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03984674, 0.02145594, 0.0164751 , 0.02222222])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.beta_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "841ec01e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mention: the target size is greater than the minimum length of the data, it will set the size as the minimum length of the data\n",
      "making the length 3's MEME output use the time 0.1671743392944336\n",
      "making the length 4's MEME output use the time 0.17180752754211426\n",
      "making the length 5's MEME output use the time 0.25025105476379395\n",
      "making the length 6's MEME output use the time 0.3084275722503662\n",
      "making the length 7's MEME output use the time 0.7732706069946289\n",
      "making the length 8's MEME output use the time 0.5844018459320068\n",
      "making the length 9's MEME output use the time 0.9474921226501465\n",
      "making the length 10's MEME output use the time 0.16924285888671875\n",
      "making the length 11's MEME output use the time 0.1607062816619873\n",
      "making the length 12's MEME output use the time 0.17380309104919434\n",
      "making the length 13's MEME output use the time 0.19881176948547363\n",
      "making the length 14's MEME output use the time 0.13724112510681152\n",
      "making the length 15's MEME output use the time 0.1459197998046875\n",
      "making the length 16's MEME output use the time 0.1270301342010498\n",
      "making the length 17's MEME output use the time 0.13109731674194336\n",
      "making the length 18's MEME output use the time 14.997830152511597\n",
      "making the length 19's MEME output use the time 16.214032649993896\n",
      "making the length 20's MEME output use the time 0.5150184631347656\n",
      "making the length 21's MEME output use the time 0.502244234085083\n",
      "making the length 22's MEME output use the time 0.17138123512268066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qoo36\\AppData\\Local\\Temp\\ipykernel_9596\\257863928.py:148: RuntimeWarning: divide by zero encountered in log\n",
      "  freq_letter = np.log(freq_letter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making the length 23's MEME output use the time 0.1393427848815918\n",
      "making the length 24's MEME output use the time 0.12112307548522949\n",
      "making the length 25's MEME output use the time 0.10052275657653809\n",
      "making the length 26's MEME output use the time 0.08383536338806152\n",
      "making the length 27's MEME output use the time 0.08089590072631836\n",
      "making the length 28's MEME output use the time 0.07557892799377441\n",
      "making the length 29's MEME output use the time 0.05318450927734375\n",
      "making the length 30's MEME output use the time 0.047606706619262695\n",
      "doing the 0 step in for loop\n",
      "doing the 1 step in for loop\n",
      "doing the 2 step in for loop\n",
      "doing the 3 step in for loop\n",
      "doing the 4 step in for loop\n",
      "doing the 5 step in for loop\n",
      "doing the 6 step in for loop\n",
      "doing the 7 step in for loop\n",
      "doing the 8 step in for loop\n"
     ]
    }
   ],
   "source": [
    "src, tgt = get_batch(40,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d3365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
