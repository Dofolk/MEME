{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The model is designed to analyze the consensus sequences in DNA sequences\n",
    "\n",
    "The input of this model will be in the 2D matrix:    \n",
    "    dimension: n x 1000, n is the amount of the input samples\n",
    "\n",
    "The output of the model will be in the 3D matrix:\n",
    "    dimension: n x 1000 x 4, n is the amoutn of the input samples\n",
    "               the meaning of the output is the position-wise appearance for each alphabets(ATCG)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac6800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer,\\\n",
    "    TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.nn.functional import softmax\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc9d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global variables\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "alphabet_dict = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4}\n",
    "\n",
    "combi_letters = {'A' : 1, 'T' : 2, 'C' : 3, 'G' : 4, \n",
    "                 'AT' : 5, 'AC': 6, 'AG' : 7, 'TC' : 8, 'TG' : 9, \n",
    "                 'CG' : 10, 'ATC' : 11, 'ATG' : 12, 'ACG' : 13,\n",
    "                 'TCG' : 14, 'ATCG' : 15}\n",
    "val_to_char = {1 : 'A', 2 : 'T', 3 : 'C', 4 : 'G'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the fasta files and make the list of DNA sequences\n",
    "def read_file(path):\n",
    "    f = open('../test.fa','r')\n",
    "    seq = {}\n",
    "\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            name = line.replace('>', '').split()[0]\n",
    "            seq[name] = ''\n",
    "        else:\n",
    "            seq[name] += line.replace('\\n','').strip()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    seq_ls = []\n",
    "    for name in seq:\n",
    "        seq_ls.append(seq[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1833835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "dropout = 0.1\n",
    "max_len = 500\n",
    "nhead = 8\n",
    "encoder_layer_nums = 6\n",
    "decoder_layer_nums = 6\n",
    "dim_ff = 512\n",
    "# token: ATCG(1~4), <BOS or SOS>(0), <EOS>(16), <PAD>(17)\n",
    "    # combinations: AT, AC, AG, TC, TG, CG, ATC, ATG, ACG, TCG, ATCG(5~15, total 11)\n",
    "ntoken = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d8495e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #done\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        nhead,\n",
    "        encoder_layer_nums,\n",
    "        decoder_layer_nums,\n",
    "        dim_ff,\n",
    "        ntoken,\n",
    "    ):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pos_encode = PositionalEmbedding(d_model, dropout, max_len)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, encoder_layer_nums)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first = True)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, decoder_layer_nums)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.out_embed = nn.Embedding(ntoken, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, ntoken)\n",
    "        self.output_softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        src_mask = None,\n",
    "        tgt_mask = None,\n",
    "        memory_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None,\n",
    "        memory_key_padding_mask = None,\n",
    "    ):\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        \n",
    "        src = self.pos_encode(src)\n",
    "        tgt = self.pos_encode(tgt)\n",
    "        \n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, \n",
    "                              tgt_mask = tgt_mask, \n",
    "                              memory_mask = memory_mask, \n",
    "                              tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask = memory_key_padding_mask\n",
    "                             )\n",
    "        output = self.output_linear(output)\n",
    "        output = self.output_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0b95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module): #done\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp( torch.arange(0, d_model, 2) * (-math.log(10000) / d_model) )\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.pe[0, :x.size(1), :].requires_grad_(False)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8b495dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEME(object):\n",
    "    def __init__(self, source_data, W, beta = 0.1):\n",
    "        '''\n",
    "        source_data will be list of strings\n",
    "        '''\n",
    "        \n",
    "        self.source_data = source_data\n",
    "        self.W = W\n",
    "        self.L = 4\n",
    "        self.beta = beta\n",
    "        \n",
    "        # fixed variables\n",
    "        self.N = None\n",
    "        self.l = None\n",
    "        self.X = None\n",
    "        self.n = None\n",
    "        self.I = None\n",
    "        self.letter_counts = None\n",
    "        self.beta_i = np.zeros((1,self.L))\n",
    "        \n",
    "        # mutable variables, i.e. the value will be changed\n",
    "        self.p_X1 = None\n",
    "        self.p_X2 = None\n",
    "        self.Z = None\n",
    "        self.z = None\n",
    "        self.f_ij = None\n",
    "        self.e_ij = None\n",
    "        self.lambda1 = 0\n",
    "        self.lambda2 = 0\n",
    "        \n",
    "        self.init_variables()\n",
    "        \n",
    "        \n",
    "    def init_variables(self):\n",
    "        '''\n",
    "        initialize the followings: X, f_i, z, I, lambda, N, small L, \n",
    "        '''\n",
    "        \n",
    "        # record N and small L, and also init beta_i\n",
    "        self.N = len(self.source_data)\n",
    "        self.l = list()\n",
    "        for seq in self.source_data:\n",
    "            self.l.append(len(seq))\n",
    "            for s in seq:\n",
    "                self.beta_i[0][alphabet_dict[s] - 1] += 1\n",
    "        \n",
    "        # trans beta_i as frequency and multi with customed beta\n",
    "        self.beta_i /= np.sum(self.l)\n",
    "        self.beta_i *= self.beta\n",
    "                \n",
    "        # init small z\n",
    "        self.z = np.ones((self.N, max(self.l)))\n",
    "        for idx in range(self.N):\n",
    "            self.z[idx][(self.l[idx]- self.W + 1):] = 0\n",
    "        self.e_ij = self.z.copy()\n",
    "        \n",
    "        # init W-mer set X\n",
    "        # X will be the list of strings\n",
    "        X_ls = list()\n",
    "        for i in range(self.N):\n",
    "            X_ls += [ self.source_data[i][j:j+self.W] for j in range( self.l[i] - self.W + 1 ) ]\n",
    "        self.X = X_ls\n",
    "        self.n = len(self.X)\n",
    "        \n",
    "        # init indicator\n",
    "        self.I = self.indicator_function()\n",
    "        \n",
    "        # init lambda\n",
    "        lamb_range_min = min( np.sqrt(self.N)/self.n, 1/(2*self.W) )\n",
    "        lamb_range_max = max( np.sqrt(self.N)/self.n, 1/(2*self.W) )\n",
    "        self.lambda1 = np.random.uniform(lamb_range_min, lamb_range_max)\n",
    "        self.lambda2 = 1 - self.lambda1\n",
    "        \n",
    "        # init f_ij, size: ( 1 + W ) x L\n",
    "        self.f_ij = np.zeros( ( (self.W + 1), self.L ) )\n",
    "        \n",
    "        # init letter_counts\n",
    "        self.letter_counts = self.count_letter_appearance()\n",
    "        \n",
    "        # f_0 part\n",
    "        count_num = defaultdict(int)\n",
    "        for subseq in self.X:\n",
    "            for s in subseq:\n",
    "                count_num[s.upper()] += 1\n",
    "        total = np.concatenate(self.letter_counts).sum()\n",
    "        for letter in alphabet_dict:\n",
    "            self.f_ij[0][alphabet_dict[letter] - 1] = count_num[letter]/total\n",
    "        \n",
    "        # f_i parts\n",
    "        for pos in range(self.W):\n",
    "            C = Counter(self.I.transpose()[pos])\n",
    "            length = len(self.I)\n",
    "            for val in range(4):\n",
    "                self.f_ij[pos + 1][val] = C[val + 1] / length\n",
    "        \n",
    "        # conditional probabilities p_X1, p_X2\n",
    "        \n",
    "        self.p_X1, self.p_X2 = self.condi_distribution(self.f_ij)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def indicator_function(self): #done\n",
    "        '''\n",
    "        In article, it is the I(k,a) function for eq(7), (8)\n",
    "        There will transfer the alphabets to the index\n",
    "        Thus, the results look like \n",
    "        [\n",
    "        [1,2,3,4],\n",
    "        [4,3,2,1]\n",
    "        ]\n",
    "\n",
    "        return\n",
    "        indicator: transfer input string into tensor indicator, size is n x W\n",
    "        '''\n",
    "        assert isinstance(self.X, list), 'Type of X is not list'\n",
    "        indicator = list()\n",
    "        for seq in self.X:\n",
    "            indicator.append(list(map(lambda x: alphabet_dict[x], seq)))\n",
    "\n",
    "        return np.array(indicator, dtype = 'int')\n",
    "        \n",
    "    \n",
    "    def condi_distribution(self, freq_letter):\n",
    "        '''\n",
    "        Calculate the conditional distribution p(Xi | theta_j)\n",
    "        eq(7),(8) in the MEME article\n",
    "        To avoid the computation error for the digits, it will use ln() to make it being summation\n",
    "    \n",
    "        Arguments:\n",
    "    \n",
    "        freq_letter: the frequences for each letter in each position, size: (W + 1) x L\n",
    "                     background ( 1 x L ) + motif ( W x L )\n",
    "                     dtype: np.array\n",
    "        ====================================================\n",
    "        return: it will be the log form output\n",
    "    \n",
    "        p_Xi_1: conditional distribution of motif sequence, size: n x 1\n",
    "        p_Xi_2: conditional distribution of background, size: n x 1\n",
    "        '''\n",
    "    \n",
    "        p_Xi_1 = np.zeros(self.n)\n",
    "        p_Xi_2 = np.zeros(self.n)\n",
    "        f_0 = freq_letter[0]\n",
    "        f_j = freq_letter[1:]\n",
    "    \n",
    "        for subseq in range(self.n):\n",
    "            for pos in range(self.W):\n",
    "                p_Xi_1[subseq] += np.log( f_j[pos][self.I[subseq][pos] - 1] )\n",
    "                p_Xi_2[subseq] += np.log( f_0[self.I[subseq][pos]-1])\n",
    "        \n",
    "        return np.exp(p_Xi_1), np.exp(p_Xi_2)\n",
    "    \n",
    "    def count_letter_appearance(self): #done\n",
    "        '''\n",
    "        count the total appearance times for each alphabets\n",
    "        Arguments:\n",
    "        data: input source W-mer data, size: n x W\n",
    "        return:\n",
    "        count: counting results, size: n x 4\n",
    "        '''\n",
    "\n",
    "        count = np.zeros( (self.n, 4) , dtype = 'int')\n",
    "\n",
    "        for i in range(self.n):\n",
    "            C = Counter(self.X[i])\n",
    "            count[i][0] = C['A']\n",
    "            count[i][1] = C['T']\n",
    "            count[i][2] = C['C']\n",
    "            count[i][3] = C['G']\n",
    "\n",
    "        return count\n",
    "    \n",
    "    def update_erasing(self):\n",
    "        '''\n",
    "        compute the erasing values e_ij\n",
    "        '''\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            for j in range(self.l[i]):\n",
    "                lower_bound = j - self.W + 1 if j - self.W + 1 >= 0 else 0\n",
    "                vals = self.z[i][lower_bound:j]\n",
    "                if sum(vals) > 1:\n",
    "                    vals /= sum(vals)\n",
    "                if any(vals > 9.99999e-1):\n",
    "                    self.e_ij[i][j] *= 1e-6\n",
    "                else:\n",
    "                    self.e_ij[i][j] *= np.exp( sum( np.log( np.ones(len(vals)) - vals ) ) )\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def update_z(self):\n",
    "        '''\n",
    "        update the small z values\n",
    "        '''\n",
    "        \n",
    "        assert sum(self.l)-self.N*(self.W - 1) == self.n, 'Error: index division not equal for updating z.'\n",
    "        idx = 0\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.l[i] - self.W + 1):\n",
    "                self.z[i][j] = self.Z[idx][0]\n",
    "                idx += 1\n",
    "        \n",
    "        return                \n",
    "    \n",
    "    def E_step(self): #done\n",
    "        '''\n",
    "        calculate the Z_ij, in article's eq(4)\n",
    "        also update small z depends on the results of Z\n",
    "\n",
    "        Arguments:\n",
    "        condi_dis: conditional distribution, from the defined function, size: n x 2\n",
    "        lamd: probability for using models, size: 1 x 2\n",
    "        return:\n",
    "        Z: membership probability, size: n x 2\n",
    "        '''\n",
    "        \n",
    "        # regulate the shape\n",
    "        p = np.array([self.p_X1, self.p_X2]).transpose()\n",
    "        lamb = np.array([self.lambda1, self.lambda2]).reshape(1, 2)\n",
    "        assert p.shape[0] == self.n, 'dimension error for E-step prob'\n",
    "\n",
    "        multi_results = p * np.tile(lamb, (self.n, 1))\n",
    "        summation = np.sum(multi_results, axis = 1, keepdims = True)\n",
    "        Z = multi_results / summation\n",
    "        self.Z = Z.copy()\n",
    "        self.update_z()\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def M_step(self):\n",
    "        '''\n",
    "        calculate the lambda and f_ij\n",
    "        \n",
    "        Arguments:\n",
    "        Z: membership from E-step, size: n x 2\n",
    "        I: indicator function, size: n x W\n",
    "        count: count the appearance time for each alphabet in every sequences, size: n x 4\n",
    "        \n",
    "        return:\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        Z = self.Z.transpose()\n",
    "        count = self.letter_counts.transpose()\n",
    "\n",
    "        # update lambda, eq(5)\n",
    "        lamb = np.mean(Z, axis = 1)\n",
    "        \n",
    "        # update f_ij\n",
    "        # calculate c_0k and c_jk\n",
    "        c_0k = np.zeros((1, self.L))\n",
    "        c_jk = np.zeros((self.W, self.L))\n",
    "        \n",
    "        # calculate the c_0k\n",
    "        for k in range(self.L):\n",
    "            for i in range(self.n):\n",
    "                for j in range(self.W):\n",
    "                    c_0k[0][k] += Z[1][i] if self.I[i][j] == (k + 1) else 0\n",
    "        \n",
    "        '''for i in range(4):\n",
    "            c_0k[0][i] = np.sum( Z[1] * count[i] )'''\n",
    "        \n",
    "        # calculate the c_jk\n",
    "        # first make erasing being 1 x n\n",
    "        E = np.hstack([self.e_ij[i][:(self.l[i] - self.W + 1)] for i in range(self.N)])\n",
    "        EZ = E * Z[0]\n",
    "        \n",
    "        for pos in range(self.W):\n",
    "            for i in range(self.n):\n",
    "                c_jk[pos][self.I[i][pos] - 1] += EZ[i]\n",
    "        \n",
    "        # start updating f\n",
    "        # won't directly update self.f since it will use the previous values to check the disparity\n",
    "        c_0k += self.beta_i\n",
    "        c_0k /= (np.sum(c_0k, axis = 1) + self.beta)\n",
    "        \n",
    "        for pos in range(self.W):\n",
    "            c_jk[pos] += self.beta_i.squeeze(0)\n",
    "            c_jk[pos] /= (np.sum(c_jk[pos]) + self.beta)\n",
    "            \n",
    "        f_ij = np.vstack((c_0k,c_jk))\n",
    "        \n",
    "        self.update_erasing()\n",
    "        \n",
    "        return lamb, f_ij\n",
    "    \n",
    "    def update_variables(self, lamb, f_ij):\n",
    "        '''\n",
    "        update the variables' values after M step\n",
    "        it will update: f_ij, lambda, conditional distribution p_X1 and p_X2, erasing\n",
    "        \n",
    "        Inputs:\n",
    "        lamb: lambda, size: 1 x 2\n",
    "        f_ij: the updated values for position-wise letter frequencies, size: (W+1) x L\n",
    "        '''\n",
    "        \n",
    "        self.lambda1 = lamb[0]\n",
    "        self.lambda2 = lamb[1]\n",
    "        self.f_ij = f_ij.copy()\n",
    "        self.p_X1, self.p_X2 = self.condi_distribution(self.f_ij)\n",
    "        self.update_erasing()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def iter(self, epsilon = 1e-6):\n",
    "        '''\n",
    "        iterate the E and M steps\n",
    "        '''\n",
    "        \n",
    "        err_ls = list()\n",
    "        f_ls = list()\n",
    "        f_ls.append(self.f_ij)\n",
    "        \n",
    "        for step in range(1000):\n",
    "            condi_dis = np.array([self.p_X1, self.p_X2])\n",
    "            lamb = np.array([self.lambda1, self.lambda2])\n",
    "            new_Z = self.E_step()\n",
    "            new_lambda, new_f_ij = self.M_step()\n",
    "            f_ls.append(new_f_ij)\n",
    "            \n",
    "            err = np.sqrt(np.sum( (self.f_ij - new_f_ij) ** 2  , axis = 1))\n",
    "            err_ls.append(err)\n",
    "            \n",
    "            if all(err < epsilon):\n",
    "                #print('At step {}, EM converges to a solution.'.format(step))\n",
    "                #print('err is ', err)\n",
    "                self.update_variables(new_lambda, new_f_ij)\n",
    "                break\n",
    "            \n",
    "            '''if step % 100 == 0 and step > 0:\n",
    "                print('in step {} err is {}'.format(step, err))\n",
    "            '''\n",
    "            self.update_variables(new_lambda, new_f_ij)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return #err_ls, new_f_ij, f_ls\n",
    "    \n",
    "    def show_seq(self):\n",
    "        '''\n",
    "        select the motif\n",
    "        '''\n",
    "        \n",
    "        #spec = np.log(self.f_ij[1:]/self.f_ij[0])\n",
    "        t = np.log((1-self.lambda1)/self.lambda1)\n",
    "        s = np.log(self.p_X1/self.p_X2)\n",
    "        motifs = list( compress(self.X, s > t) )\n",
    "        \n",
    "        return motifs\n",
    "    \n",
    "    def prob_out(self):\n",
    "        '''\n",
    "        output the last probability results for motifs\n",
    "        '''\n",
    "        \n",
    "        motifs = self.show_seq()\n",
    "        pos_prob = np.zeros((self.W, self.L))\n",
    "        logo_tmp = [list() for _ in range(self.W)]\n",
    "        \n",
    "        for seq in motifs:\n",
    "            for i in range(len(seq)):\n",
    "                pos_prob[i][ alphabet_dict[seq[i]] - 1 ] += 1\n",
    "                if seq[i] not in logo_tmp[i]:\n",
    "                    logo_tmp[i].append(seq[i])\n",
    "        \n",
    "        pos_prob /= len(motifs)\n",
    "        \n",
    "        # sort the logo\n",
    "        for i in range(len(logo_tmp)):\n",
    "            if len(logo_tmp[i]) > 1:\n",
    "                sorted_logo = sorted(list(map(lambda x: alphabet_dict[x], logo_tmp[i])))\n",
    "                res = list(map(lambda x: val_to_char[x], sorted_logo))\n",
    "                logo_tmp[i] = res\n",
    "\n",
    "        logo = list()\n",
    "        for l in logo_tmp:\n",
    "            logo.append(''.join(l))\n",
    "        \n",
    "        return pos_prob, logo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51eb9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seq(input_seq, IO, pad_num = 17):\n",
    "    '''\n",
    "    adjust the length for single sequence\n",
    "    tokenize the sequence\n",
    "    '''\n",
    "    \n",
    "    L = len(input_seq)\n",
    "    \n",
    "    if IO:\n",
    "        if L > 498:\n",
    "            mid = L//2\n",
    "            seq = input_seq[mid - 249: mid + 249]\n",
    "            res = np.array(list(map(lambda x: combi_letters[x], seq)))\n",
    "            res = np.concatenate((np.array([0]), res, np.array[16]))\n",
    "        else:\n",
    "            res = np.array(list(map(lambda x: combi_letters[x], input_seq)))\n",
    "            res = np.concatenate((np.array([0]), res, np.array([16]), np.array([pad_num] * (498 - L) )))\n",
    "        assert len(res) == 500, 'Preprocess the input sequence not equal to 500'\n",
    "    \n",
    "    else:\n",
    "        assert L <= 48, 'Preprocess the output sequence get the length error, > 48'\n",
    "        res = np.array(list(map(lambda x: combi_letters[x], input_seq)))\n",
    "        res = np.concatenate((np.array([0]), res, np.array([16]), np.array([pad_num] * (48 - L))))\n",
    "    \n",
    "    return res.astype('int')\n",
    "\n",
    "def make_outputs(data, row_size):\n",
    "    '''\n",
    "    make the outputs for the model, i.e. make the tgt\n",
    "    return:\n",
    "    tgt: the target array, has transfered the string data into tokens\n",
    "    '''\n",
    "    tgt = list()\n",
    "    l = list()\n",
    "    for seq in data:\n",
    "        l.append(len(seq))\n",
    "    assert min(l) > 3, 'The minimum length of the inputs are less than 3'\n",
    "    if min(l) < row_size:\n",
    "        row_size = min(l)\n",
    "        print(\"Mention: the target size is greater than the minimum length of the data, it will set the size as the minimum length of the data\")\n",
    "    \n",
    "    for motif_len in range(row_size - 3):\n",
    "        #print('At step ', motif_len)\n",
    "        m = MEME(data, motif_len + 3)\n",
    "        m.iter()\n",
    "        motif, logo = m.prob_out()\n",
    "        tgt.append(preprocess_seq(logo, 0))\n",
    "    tgt = np.array(tgt)\n",
    "    assert len(tgt.shape) == 2, 'tgt dimension error in make outputs'\n",
    "    return tgt\n",
    "\n",
    "def expand_dim(data, IO, pad_num = 17, row_size = 40):\n",
    "    '''\n",
    "    To expand input and output data into specific dimensions\n",
    "    The input variable of this function is transfered matrix, that is the matrix with tokenized elements\n",
    "    return:\n",
    "    res: the results of expanding the dimension\n",
    "    '''\n",
    "\n",
    "    row, col = data.shape\n",
    "    assert row < row_size, 'Row size out of range.'\n",
    "\n",
    "    if IO:# expand input to row_size x 500\n",
    "        assert col <= 500, 'Column size out of range(Input)'\n",
    "        col_expand = pad_num * np.ones((row, 500 - col))\n",
    "        row_expand = pad_num * np.ones((row_size - row, 500))\n",
    "\n",
    "    elif not IO:# expand output to row_size x 50\n",
    "        assert col <= 50, 'Column size out of range(Output)'\n",
    "        col_expand = pad_num * np.ones((row, 50 - col))\n",
    "        row_expand = pad_num * np.ones((row_size - row, 50))\n",
    "\n",
    "    res = np.concatenate( (np.concatenate((data, col_expand), axis = 1), row_expand), axis = 0)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_padding_mask(self, data, pad_num = 7):\n",
    "    mask = torch.zeros(data.size())\n",
    "    mask[data == pad_num] = -torch.inf\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea508d39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mention: the target size is greater than the minimum length of the data, it will set the size as the minimum length of the data\n"
     ]
    }
   ],
   "source": [
    "data = ['ATCGGGTTC','ATTTGCGTC']\n",
    "tgt = make_outputs(data, 50)\n",
    "ex = expand_dim(tgt,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "748630f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = TransformerModel(d_model, dropout, max_len, nhead, encoder_layer_nums, decoder_layer_nums, dim_ff, ntoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, model, data_in, W, row_size = 40):\n",
    "        \n",
    "        self.model = model\n",
    "        self.data = data_in\n",
    "        self.row_size = row_size\n",
    "        self.W = W\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        #tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb98840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    path = ''\n",
    "    mdl = TransformerModel()\n",
    "    \n",
    "    # read data\n",
    "    data = read_file(path)\n",
    "    # build model and train\n",
    "    \n",
    "    T = trainer()\n",
    "    T.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['CAGTCAGCAC','AGTTACGTAG']\n",
    "data1 = ['AAAAA','AAAAA']\n",
    "m = MEME(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = TransformerModel(d_model, dropout, max_len, nhead,\n",
    "                       encoder_layer_nums, decoder_layer_nums,\n",
    "                       dim_ff, ntoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5,6]\n",
    "])\n",
    "\n",
    "tgt = torch.LongTensor([\n",
    "    [0,3,2,4,3,3,2,1,5,6],\n",
    "    [0,4,3,1,2,3,1,2,1,5]    \n",
    "])\n",
    "\n",
    "mdl(src,tgt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = mdl.embedding\n",
    "mdl.decoder(emb(tgt),emb(src)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8152373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
